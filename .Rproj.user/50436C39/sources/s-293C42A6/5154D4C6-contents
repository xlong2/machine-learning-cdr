%%
%% Copyright (c) 2015, CV Radhakrishnan, <cvr@river-valley.org>
%% 
%% This file is part of the 'PeerJ Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'PeerJ Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for PeerJ document class `peerj.cls'
%%
\documentclass[review]{peerj}

\usepackage{tabulary}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Following additional macros are required to function some 
% functions which are not available in the class used.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{url,multirow,morefloats,floatflt,cancel,tfrupee}
\makeatletter
\AtBeginDocument{\@ifpackageloaded{textcomp}{}{\usepackage{textcomp}}}
\makeatother
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage[nointegrals]{wasysym}
\urlstyle{rm}
\makeatletter

\AtBeginDocument{
\expandafter\ifx\csname eqalign\endcsname\relax
\def\eqalign#1{\null\vcenter{\def\\{\cr}\openup\jot\m@th
  \ialign{\strut$\displaystyle{##}$\hfil&$\displaystyle{{}##}$\hfil
      \crcr#1\crcr}}\,}
\fi
}

\let\lt=<
\let\gt=>
\def\processVert{\ifmmode|\else\textbar\fi}
\let\processvert\processVert

\@ifundefined{subparagraph}{
\def\subparagraph{\@startsection{paragraph}{5}{2\parindent}{0ex plus 0.1ex minus 0.1ex}%
{0ex}{\normalfont\small\itshape}}%
}{}

% These are now gobbled, so won't appear in the PDF.
\newcommand\role[1]{\unskip}
\newcommand\aucollab[1]{\unskip}
  
\@ifundefined{tsGraphicsScaleX}{\gdef\tsGraphicsScaleX{1}}{}
\@ifundefined{tsGraphicsScaleY}{\gdef\tsGraphicsScaleY{.9}}{}
% To automatically resize figures to fit inside the text area
\def\checkGraphicsWidth{\ifdim\Gin@nat@width>\textwidth
	\tsGraphicsScaleX\textwidth\else\Gin@nat@width\fi}

\def\checkGraphicsHeight{\ifdim\Gin@nat@height>.9\textheight
	\tsGraphicsScaleY\textheight\else\Gin@nat@height\fi}

\def\fixFloatSize#1{\@ifundefined{processdelayedfloats}{\setbox0=\hbox{\includegraphics{#1}}\ifnum\wd0<\columnwidth\relax\renewenvironment{figure*}{\begin{figure}}{\end{figure}}\fi}{}}
\let\ts@includegraphics\includegraphics

\def\inlinegraphic[#1]#2{{\edef\@tempa{#1}\edef\baseline@shift{\ifx\@tempa\@empty0\else#1\fi}\edef\tempZ{\the\numexpr(\numexpr(\baseline@shift*\f@size/100))}\protect\raisebox{\tempZ pt}{\ts@includegraphics{#2}}}}

%\renewcommand{\includegraphics}[1]{\ts@includegraphics[width=\checkGraphicsWidth]{#1}}
\AtBeginDocument{\def\includegraphics{\@ifnextchar[{\ts@includegraphics}{\ts@includegraphics[width=\checkGraphicsWidth,height=\checkGraphicsHeight,keepaspectratio]}}}

\def\URL#1#2{\@ifundefined{href}{#2}{\href{#1}{#2}}}

%%For url break
\def\UrlOrds{\do\*\do\-\do\~\do\'\do\"\do\-}%
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
\def\floatpagefraction{0.8} 
\def\dblfloatpagefraction{0.8}
\def\style#1#2{#2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Preprint option to format the sources to suit Preprint server
%% \documentclass[preprint]{peerj}

%%
%% Final option to format sources look like final published articles
%% \documentclass[final]{peerj}

%% Use the following line if you want the final typeset format
%% \documentclass[final]{peerj}

%% For including figures, graphicx.sty has been loaded in
%% peerj.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
%% \usepackage{amssymb}

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% Use a bibliography/citation package for numbered style
%%
%% \usepackage[sort@compress,numbers]{natbib}
%%
%% Use a bibliograpphy/citation for author year style
%%
%% \usepackage[longnamesfirst,round]{natbib}
%% 

%% The lineno packages adds line numbers. It is automatically loaded
%% along with preprint or review option. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% 



\usepackage{textcase}
\makeatletter

\patchcmd{\section}{\MakeUppercase}{\MakeTextUppercase}{}{}
\makeatother

\journal{PeerJ Computer Science}

		
    
    




\begin{document}

%% Title, authors and addresses

%% use the tnotemark command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnmark command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the cormark command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title}
%% \tnotemark[1]
%% \tnotetext[1]{}
%%
%% \author{Given names Surname}
%%     or
%% \author[<address num>]{Given names Surname}
%% \cormark[1]
%% \ead{email address}
%% \ead[url]{home page}
%% \fnmark[1]
%% \fntext[1]{footnotetext}
%% \cortext[1]{}
%%
%% \address{Address}
%%     or
%% \address[<num>]{Address}

\title{Improving non-H3 CDR template selection in antibody modeling}
  
    

\begin{abstract}
Antibodies are diverse proteins that can be rapidly generated by the adaptive immune system to recognize and counteract specific pathogens. While these features make antibody molecules ideal therapeutics, they also make accurate computational modeling challenging. In antibody modeling, the framework regions(FR) are well conserved and readily modeled to sub-Angstrom accuracy, but accurate modeling of the complementary determining region (CDR) loops remains elusive. Both homology and \textit{de novo} modeling have been used for antibody complementarity determining region (CDR) modeling. Homology modeling has achieved greater accuracy than the \textit{de novo} modeling for the non-H3 loops to date. The greater success of homology modeling in non-H3 CDR loops can be attributed to the shorter loop length, less structural variability. Indeed, most of the non-H3 CDR loop structures can be grouped by CDR type(L1, L2,..) and loop length and can be clustered into a few canonical structure clusters. Four major antibody modeling suites, Kotai SabPred, Rosetta Antibody, PIGS vary in their utilization of canonical structure clusters in non-H3 CDRs modeling, PIGS and Kotai utilize sequence rules that can identify cluster membership for a fraction of CDRs, with Kotai also usse PSSM scoring for more thorough cluster assignment. Rosetta and SabPred do not utilize canonical clusters during CDRs homology modeling explicitly. Current studies comparing the CDR modeling qualities of these methods do not address to the quality of initial structure template selection, nor are the studies performed on a comprehensive dataset. In this study, we assessed the initial template selection quality using the RosettaAntibody method in whether the structure templates selected are in the same cluster as the CDR query on the PyIgClassify dataset. We also trained gradient boosting tree(GBM) models to do the prediction rather using any preset sequence rules or implicitly identify the cluster as current Rosetta Antibody, improving the accuracy from 81\% to 85\%. The method is especially useful for distinguishing certain cluster pairs which RosettaAntibody has been bad at. And we find the improvement is related to the member size distribution between the cluster pair, the data sparsity in the loop and length type, the between-clusters structural distances and the base blindBLAST performance. Based on the factors identified, we suggest possible ways for further improving the prediction accuracy in future studies. 


\end{abstract}
\begin{keywords}
    
\end{keywords}
	
    
\maketitle
    
    
\section{Introduction}
Antibody molecules are central to adaptive immunity. They are responsible for recognizing a variety of target molecules known as antigen. They achieve the ability to recognize any one of a diverse set of targets through two biological mechanisms: V(D)J recombination and somatic hypermutation during. These gene editing mechanisms can produce an enormous quantity of unique sequences, on the order of 10\ensuremath{^{13}} (Pedotti et al., 2011). Recent advances in high-throughput sequencing techniques have permitted unparalleled access to the human antibody repertoire(Boyd, Crowe \& Jr., 2016), furthering understanding of immune response to vaccination(Luciani, 2016), infection(Hou et al., 2016) , and autoimmunity(Sarkar et al., 2007). Additional insights about antibody function can be provided by structural information. Yet, there exists only a very small fraction of antibodies with solved crystal structures in the Protein Databank, on the order of 10\ensuremath{^{4,}}(Dunbar et al., 2014). It would be challenging and time consuming to close this gap in knowledge using thorough experimental structure determination methods. Computational modeling could provide a feasible alternative: modeling of antibody structures has shown to add to the prognostic value of sequence data alone in chronic lymphocytic leukemia(Marcatili et al., 2014). Beyond using modeling to develop biological understanding, docking studies of antibodies complexed with various antigens can reveal atomic details of antibody{\textendash}antigen interactions(Koivuniemi, Takkinen \& Nevanen, 2017)\ensuremath{^{,}}(Kilambi \& Gray, 2017). Finally, in antibody design studies, computational approaches can utilize a variety of sequence searching protocols to enhance affinity or design an antibody \textit{de novo}{\textemdash}without prior sequence information(Lippow, Wittrup \& Tidor, 2007; Dunbar et al., 2016a; Baran et al., 2017; Weitzner et al., 2017a) . However, to be useful, computational methods must be able to accurately predict antibody structure.

\textbf{} Typical approaches to antibody structure prediction decompose the problem into three stages, based on known antibody-structural features(Kuroda et al., 2012). Antibodies are typically comprised of a light and a heavy chain, each having a variable (V) and a constant (C) region. While the constant region is important for signaling, it does not vary across antibodies and does not greatly affect antigen-binding function. On the other hand, the variable region can be quite different between antibodies and is responsible for recognizing antigen. The variable region can be further divided into a framework region (FR), with greek-key \ensuremath{\beta }-barrel topology, and complementarity determining regions (CDRs), which are solvent exposed loops connecting \ensuremath{\beta }-strands. The FR is conserved and has a low rate of mutation across antibodies whereas the CDRs, and in particular the CDR H3, are highly mutable to be able to bind a wide variety of antigen(Glanville et al.). So, the antibody modeling problem decomposed is: (1) homology modeling of light and heavy FRs, (2) homology modeling of the non-H3 CDR loops, and (3) \textit{de novo } modeling of the CDR-H3 loop. 

Of the three components of antibody modeling, modeling of the CDR-H3 loop was found to be the most challenging, with an average accuracy of 2.8\ensuremath{\pm}0.4 {\AA} reported over eleven test antibodies and seven modeling approaches in a recent blind assessment(Almagro et al., 2014). By comparison, FR modeling was quite accurate, averaging sub-angstrom accuracy for both the light and heavy chains. Modeling of the non-H3 CDRs was also found to be challenging with average accuracy ranging from 0.5\ensuremath{\pm}0.1 to 1.3\ensuremath{\pm}1.1 {\AA} for RosettaAntibody in the same assessment(Weitzner et al., 2014). This was surprising since the non-H3 CDR loops have canonical conformations. Previous studies have found that, when divided by loop type and length, a majority (85\%)(North, Lehmann \& Dunbrack, 2011) of non-H3 CDRs assume similar structures to just a few structures, termed canonical conformations. However, whether antibody modeling methods have been effectively utilizing this structural information is still a question to be answered.

We examined four methods SabPred(Dunbar et al., 2016b), PIGS(Marcatili et al., 2014) Kotai(Yamashita et al., 2014) and Rosetta Antibody(Weitzner et al., 2017b). In all four methods, non-H3 CDR loops are modeled by homology: a loop of similar sequence is chosen as a template structure based on a query CDR loop sequence. Template selection varies across methods in mainly two aspects. 

The first aspect is how or if at all they utilize clustering information for template selection. Out of the four aforementioned methods, only PIGS and Kotai Antibody Builder utilize the structural cluster for template selection by predicting whether a query sequence belongs to a specific structural cluster; If a cluster is identified, the methods constrain the template search within the identified clusters.In terms of the methods for cluster identification, PIGS and Kotai both use sequence based rules to identify the structural clusters from the query CDR sequence. While sequence rules can offer deterministic cluster  assignment and are easy for human interpretation, they are limited in their adaptability and power{\textemdash}as the number of known antibody structures and sequence grows, analysis by hand becomes more and more challenging. Furthermore, some clusters are devoid of human-identified, deterministic rules. The CDRs that can't be retrieved by such rules makes the cluster identification using sequence rules to be an incomplete method. Using the current PIGS method as the example, it uses curated rules from a variety of previous studies(Marcatili et al., 2014), in which there are 4 canonical clusters from 4 different loop lengths with sequence rules. However, the North study identified the presence of 17 structural clusters from 6 loop lengths in H1 loops(North, Lehmann \& Dunbrack, 2011). Another method Kotai(Yamashita et al., 2014) utilize the set of sequence rules for cluster identification updated in the North study, but the North study shows no sequence rules for identifying H1 clusters and rules for only a fraction of the remaining non-H3 clusters (26/56)(Shirai et al., 2014). On top of this, some sequence rules address only a fraction of the structures in clusters, such as R residue on the 71 residue based on Chothia numbering scheme(Chothia et al., 1989) on the framework region are uniquely present in 8 out of 155 CDRs in H2-10-1 and 38 out of 42 CDRs in H2-10-2, but not in the remaining structures in H2-10-1 and H2-10-2. Kotai Antibody Builder addressed this problem by devising a two voter method to incorporate all canonical structures in North study into their method other than what sequence rules can identify.(Shirai et al., 2014) If the query sequence is not covered by any sequence rules, the prebuilt position-specific-substitution-matrix (PSSM) profiles would score the sequence against all candidate clusters and assign it the clusters that passed the score thresholds. If multiple clusters are identified, the cluster with CDR of the same origin as the selected framework structure template is favored. The Kotai cluster identification method correctly identified cluster in 90\%(Shirai et al., 2014) of cluster of all CDR loops including non-H3 and H3 using PyIgClassify dataset. But, it is not clear whether the tested data is excluded from clustering the PSSM profiles. Therefore, their accuracy might be overestimated.

The second aspect is the scoring matrix used for selecting the most similar CDR out of all candidate CDRs utilized by the different methods. For sequence similarity, all methods use a substitution matrix to score for all candidate CDRs of the same length and loop type, and select one or a few CDRs that are top ranked. SabPred [10] utilizes the method FREAD(Choi \& Deane, 2011), which uses an environmentally constrained substitution matrix(Shi, Blundell \& Mizuguchi, 2001). RosettaAntibody uses the PAM30 substitution matrix, whereas PIGS and Kotai  use BLOSUM62. Once a template structure is selected, it is grafted on to the already modeled antibody framework and threaded with the query sequence. In some cases, the grafted model is then  further energy refined in a particular force field or energy function(Kuroda et al., 2012). 

 [Paper] Present studies on the CDRs homology modeling of these methods lack adequate assessment in the template selection quality using equivalent dataset. Most of the published work also focused on the quality of final model compared to the native structures without assessing the initial template selection adequately. A recent study(Weitzner et al., 2014) benchmarked the RosettaAntibody homology modeling method on 54 antibody targets, with 42/54 (L1), 50/54 (L2), 37/54 (L3), 36/54 (H1), and 42/54(H2) having less than 1 {\AA} RMSD between the homology modeled and actual CDR. One the other hand, a paper assessing PIGs(Marcatili et al., 2014) used a set of 689 antibody structures and leave-one-out-cross-validation (LOOCV) to findonly 50\% of the modeled non-H3 CDRs have less than 1 {\AA} RMSDs to the native CDRs structures. Finally, SabPred(Dunbar et al., 2016a) was tested on a set of 54 antibodies and reported larger average RMSDs in 3 out of 5 non-H3 CDRs compared to  RosettaAntibody (In Angstrom 1.09, 1.00, 0.88 versus 0.83, 0.91, 0.83 for loops L1, L3, H1).

There are mainly three problems in these studies. First, the benchmarking studies for different modeling methods are done using different sets of antibodies so evidence suggesting the superiority of one method over another is confounded by the disparity of test set size and template library size between the studies. For example, the smaller percentage of sub-angstrom accuracy models compared to Rosetta is at least partially due to the smaller template library of PIGs. Second, the studies are limited in the size of the test dataset. Evaluation on a small and incomplete data set can elide some CDR template selection problems, which might be observed without a larger test set. Third, the use of final models rather than the structural template in these evaluation, are not directly indicative to the template structure similarity to that of the native structure, because the final structures are subjected to energy refinement or other kinds of modification, and can assume a different canonical structure cluster than the initial template comes from the correct cluster.(Weitzner et al., 2014) Therefore, we need a proper assessment with a large set of test set for a fair comparison of template selection.

Because of these shortcomings present in accuracy evaluation of non-H3 CDR template selection, we assessed the quality of template selection rather than the quality of final model using the Rosetta CDR modeling method with the comprehensive dataset PyIgClassify.Based on our result of template selection cluster accuracy, we attempted to increase the accuracy using machine learning. Machine learning has been used extensively in protein classification problems such as predicting protein function, folding rate, superfamily for fold recognition, enzyme classes, functional binding sites[x1]  . Specifically, Gradient boosting Machine (GBM) is used because a recent study demonstrate that the ensemble method gives the best accuracy in the Structural Classification Of Protein database(SCOP).(Jain, Garibaldi \& Hirst, 2009) The conclusion of the advantageous performance for GBM also applies to CDR canonical cluster prediction because of the similarity between the features and the nature of classesbetween PyIgClassify and SCOP.

The integration of GBM and Rosetta utilized BLAST searching is termed as guidedBLAST. The performances between the GBM model accuracy and the cluster identification accuracy implicitly achieved by the blindBLAST in RosetttaAntibody is compared when both methods are tested on the PyIgClassify database. And the characteristics of misclassifications which GBM gained improvement are also studied.

Based on our study, we found that machine learning not only capture the important features that are marked out by previous studies, but also the important features that are not yet unique enough to assign a sequence to a specific structural cluster but in combination can render probabilities to candidate clusters so that a most probable cluster can be assigned to a query sequence, even in the absence of a sequence rule. The machine learning model also achieved greater accuracies for most of the CDR loop types except for two of the loops with small loss in accuracy. Based on this finding, we think incorporating machine learning method can achieve more close-to-native templates during non-H3 CDR homology modeling, And second, using machine learning is more automated than manual curation of sequence rules which are subject to the limit of the data at the time of the studies. \textbf{}\textbf{}


    
\section{Materials \& Methods}




\subsection{\textbf{PyIgClassification dataset}}The PyIgClassify database was used for our study. We choose this dataset because it has the most comprehensive structure cluster division and it incorporates all CDRs extracted from solved antibodies with cutoffs of 2.8 {\AA} resolution, B-factor of 80 or higher and non-Pronline \textit{cis} loops or loops with highly improbable conformations excluded(North, Lehmann \& Dunbrack, 2011). The set of non-redundant canonical CDR loops was obtained from the database, in which the CDR loops are partitioned by their CDR loop type and length. CDRs loop structures in each partition are clustered so that the members in each cluster are always more structurally similar to their cluster exemplar than to any other cluster exemplars. The distribution of CDR cluster membership is very unbalanced, with each CDR loop and length pair having one well-populated or dominant cluster and many sparsely populated or non-dominant clusters. We use the `fullcluster' cluster separation labeled in the PyIgClassify database. In our study, clusters that have no more than three members were merged into a ``none'' cluster v, and denoted as ``*'' in cluster identifiers. The overall cluster member size distribution by CDR loop and length type is shown in Figure 1.



\subsection{Random model approach} Because of the unbalanced cluster size distribution and variable numbers of total clusters in different CDR loop and length types, we constructed a null model for comparison. to simulate random cluster assignment.. Specifically in the null model result generation, cluster memberships are assigned to CDRs in PyIgClassify dataset according to the corresponding cluster membership size distribution for each CDR loop and length type in the dataset. This sampling is performed 1000 times. each time, the error cases are identified as none equivalent assignments: where the query cluster and the randomly assigned cluster differ. The empirical error count distribution for each misclassification can be obtained under the null model. An example of the random assignment error count distribution is shown in Figure 11.\textbf{}



\subsection{Blind BLAST approach}In the current version of RosettaAntibody(Weitzner et al., 2017b), to model any non-H3 CDR loop, a BLAST search to sequence similarity as determined by the PAM30 matrix in order to select the best structural templates among CDR loops of the same length and loop types, In our study, this method is referred to as ``blindBLAST'' as it does not utilize canonical CDR structural cluster information. The BLAST parameters used are ````-substitution\_matrix PAM30 -word\_size 2 -max\_target\_seqs 3000 -evalue 2000''. . Because of the same sequence length and homologous origin between query and template candidates the template selection is solely based on the a.a.-to-a.a.-corresponded substitution scores extracted from the PAM30 matrix. Each of the template structures is then grafted onto the already modeled framework and subject to further energy minimization and structural refinement. 



\subsection{blindBLAST misclassification }The CDR misclassification of the blindBLAST is constructed. BlindBLAST searching is conducted in LOOCV setting, in which candidate templates for a CDR query are all the CDRs of the same length and type, excluding the query itself. The template CDR that gives the highest similarity score is chosen as the template for each query as described in the Introduction. An error case is identified when the cluster of query and the cluster of its selected template are different. 

With the error count from blindBLAST and the null model, the bias of blindBLAST method toward cluster-to-cluster misclassifications can be evaluated using a significance test, which test the observed blindBLAST error count against the empirical error count distribution generated by the null model for each misclassification. A two-tailed hypothesis test at 0.05 significance level is formulated as the following equation: \begin{eqnarray*}\begin{array}{l}\begin{array}{l}H_0:\;\;\mathrm{observed}\;\;x_\mathrm{error}\;\mathrm{not}\;\mathrm{significantly}\;\mathrm{different}\;\mathrm{from}\;\mathrm{random}\;\mathrm{assignmnet}\;\\H_\mathrm a:\;\;\mathrm{observed}\;\;x_\mathrm{error}\;\mathrm{significantly}\;\mathrm{different}\;\mathrm{from}\;\mathrm{random}\\\alpha=0.05,\\F_{x_\mathrm{error}}=\frac{{\displaystyle\sum_{n=1}^{N_\mathrm{all}=1000}}I(x_\mathrm n^\ast\geq x_\mathrm{error})}{N_\mathrm{all}},\end{array}\\p=min(F_{x_\mathrm{error}},\;1-F_{x_\mathrm{error}}),\\if\;p\leq0.025,\;H_0\;\mathrm{is}\;\mathrm{rejected},\\\mathrm{if}\;\mathrm p>0.025,\;{\mathrm H}_0\;\mathrm{is}\;\mathrm{not}\;\mathrm{rejected}\\\end{array}(1)\end{eqnarray*}



\subsection{\textbf{How the relative distances between clusters and within clusters affect the chance of misclassifications}}Some loop and length types are observed frequently in the problematic misclassifications identified in the significance test. We picked out four of them H1-13, H2-10, L2-8 and L3-9, and examined how cluster-distances-associated factors can affect the categorization of misclassifications. The first factor is the between-exemplar-dihedral-angle distance of the two involved clusters in a specific misclassification. The between-exemplar-dihedral-angle distances for different misclassifications from these loop and length types are listed, the values for the "good" misclassifications and "bad" misclassifications are compared. The second factor is the between-query-and-cluster-exemplar-dihedral-angle distance. As a lot of the misclassifications involves a dominant cluster and a non-dominant cluster, the dominant clusters cluster-1 in these loops except for L3-9-cis7-1 have poor recoveries. In these dominant clusters, cases can be divided into blindBLAST wrong and right cases. We extracted the dihedral angle distances between the queries and exemplars of the corresponding clusters. Then, the diatnce3s are divided based on whether the templates are in the right or wrong clusters. The density plots (Figure 5) for the right cases and the wrong cases are generated individually and overlaid on the same x-axis for comparison. Also, we consider any CDRs to be in the neighborhood of a query CDR if the dihedral angle distance between them to be within 1/10th of the radius of query cluster, for each query loop, its neighborhood-structure-number in the same cluster is counted. In a similar manner, the density plot of the neighborhood-structure-number for the right cases and for the wrong cases in blindBLAST result are plotted and overlaid on the same x axis for comparison. 



\subsubsection{GBM model approach}After performing a preliminary search to find the best machine learning method, with surveyed methods including linear models, decision trees, support vector machine and gradient boosting machine(GBM), we selected GBM was chosen as the best learning method for predicting CDR clusters from sequence. Features used for prediction were CDR loop sequence and its neighboring a.a. sequence . For a CDR loop and its 10 upstream residues and 10 downstream residues, each amino acid was coded as the corresponding a.a. being 1 while the remaining 19 a.a. being 0. We also did literature search on various sampling methods for overcoming the cluster size imbalance. Previous research have evaluated methods such as down-sampling the majority class, and up-sampling the minority classes by resampling cases in these classes or adding synthetic cases to them.(Chawla et al., 2002; Blagus et al., 2013) Results suggest none of the sampling methods are always better than others but dataset and data size dependent. One study pointed out down-sampling majority classes inside each weak learner in an ensemble of weak learners gives the most robust receiver operating characteristic (ROC). The effectiveness of the method is thought to benefit from independent realizations of weak learners trained by more distinct set of samples in majority classes. [x1]  

 Ten-fold-cross-validation(CV) was used to evaluate blindBLAST can find the template which belongs to the same canonical structure cluster as the query. This scheme divides the CDRs in each loop and length into ten folds. The fold division ensures CDRs clusters composition in every query set and training set resembles that in the entire dataset.A single comprises a query(validation) set and the remaining nine folds comprise a template (training)set. The prediction accuracies in each of the query set using the prediction model trained from the corresponding training set are collected. This fold division and prediction model accuracy estimation is repeated three time. The average CDR cluster identification accuracy for each non-H3 loop is calculated. 

We further optimize the model training process by data resampling and model tuning for each loop and length type. First, to make the input data in each decision tree more likely to comprise balanced data distribution, the training data undergo an additional sampling step. In this sampling step, cases belonging to the less popular classes are resampled to match to the number of cases belonging to the most popular class. Second, to find the best model complexity, a search over a hyper-parameter grid was conducted, where each grid corresponds to a specific model complexity manifested by the number of decision trees in the model, the number of branches formed by the decision tree and the degree each tree contribute its knowledge to the model in the boosting process. For each loop and length type, the best GBM model complexity is the one hyper-parameter grid that gives the highest model estimation accuracy than that of other grids. In the same data division scheme 10-fold-cross-validation, we also evaluate how well blindBLAST can find the correct canonical structure cluster for a query sequence. The average CDR cluster identification accuracy using blindBLAST is compared to the best model estimation accuracy of the GBM model for each loop and length type.



\subsection{The feature importance extracted from GBM models} In the GBM models, the presence of residue i at position j is a feature. From the best GBM model, the important features are ranked by how much it helps to reduce the training error in a relative importance scale of 1-100. The feature with 100 value is the most important one. The absolute importance is calculated by first calculating how much a decision tree split reduce Gini impurity after the [equation XX], the summing over all node-size-weighed reductions on splits corresponding to that feature over all boosting trees as the following equations.\textbf{}\begin{eqnarray*}\begin{array}{l}\mathrm{Gini}\;\mathrm{impurity}:\;\mathrm G=1-(p_1^2+p_2^2+...+p_j^2)\;\mathrm{for}\;\mathrm{class}\;1...\mathrm j\\\mathrm{Variable}\;\mathrm{importance}:\\\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;V_\mathrm i=\sum_1^{n_\mathrm{iter}}\sum_{i=1}^{\;i=n_i}(n_\mathrm{base}\ast G_\mathrm i(\mathrm{base})-n_\mathrm{left}\ast G_\mathrm i(\mathrm{left})-n_\mathrm{right}\ast G_\mathrm i(\mathrm{right})\end{array}\end{eqnarray*}
    
\section{Results}




\subsection{Blind BLAST accuracy of cluster prediction} In the 3-repeat-10-fold-cross-validation of the blindBLAST method, accuracies vary among the different CDR loop and length with many below 0.9. Figure 3 shows there are substantial increases in query-versus-template-structure RMSD using blindBLAST versus if the queries are aligned with the most similar CDR template in their clusters. The incorrect cluster-assignment observed using blindBLAST lead to a worse query-versus-template-structure RMSD, and the most substantial ones are in the H1-13, H2-10, L1-11 L1-12, L2-8, L3-9. Therefore improving the accuracy of cluster-assignment can improve the quality of template structure.



\subsection{Blind BLAST has spectacular (worse than random) failures.}BlindBLAST cluster-assignment-accuracies are low for several loop and length types. We identified a few trends that connect the low accuracies with the properties of the clusters in the loop and length types. We find loop and length types with larger clusters have low cluster membership accuracy using blindBLAST compared to those with smaller cluster numbers. For example H1-13 has 8 clusters and below 80\% assignment accuracy whereas H1-15 and H2-9 have over 90\% assignment accuracy using BlindBLAST. L1-11 have lower accuracy than H1-13 and H2-10 for the same reason. In addition to the number of clusters, loop and length types with more balanced cluster-size-distributions have low accuracy. For example, H2-10 and H1-13 have a similar number of clusters, but H2-10 has two clusters H2-10-1 and H2-10-2 with more balanced member sizes than those of H1-13-1 and H1-13-2, resulting in low accuracy at 75\% compared to 78\% H1-13. L1-11 have smaller cluster number than L3-9, also has lower accuracy than L3-9 because of its greater cluster-member-sizes balance. Other than the total cluster number and cluster-member-size balance, loop and length types with small total sample size can have low accuracy.  Loops L1-12, L3-8, L3-10, L3-12 have the worst accuracies among all loops and dthey have both sparse sample sizes and greater cluster-member-size-balance. \textbf{}

We can intuitively understand there is a greater chance for misclassification when a loop and length type has a larger number of clusters. However, the lower accuracies in loop and length types with greater cluster-member-size balance indicates that more members in non-dominant clusters worsens blindBLAST searching accuracy. Finally, the lowest accuracies are for loop and length types with small sample sizes, suggesting that the accuracy can be limited by sparse data.  Next, we applied a statistical test (equation 3) to compare blindBLAST to the random assignment. A summary of misclassifications categorized by the significance is shown in Table 4. 

The misclassifications are divided into 4 categories as described in Table 1. Within each category except for the ``significantly worse than random'' group, misclassifications usually appear in pairs of involved cluster being the same but the order switched. Additionally, misclassifications in a category are concentrated to just a few loop and length types (Table 2). 

BLAST with PAM30 for short sequences is an established tool for homology sequence searching. However in some loop and length types, blindBLAST identifies templates with significantly higher error count than assigning clusters randomly. The cause of these lower than random-assignment performance is of our specific interest. As template selection are based on substitution scores, we examined their problematic substitution scores. The score at each loop position for the alignment to the wrong cluster was compared to the alignment to the correct cluster, where the correct alignment was obtained by template search only within the query cluster. We found five out of eight misclassification cases from L2-8-1 to L2-8-5 happened because the alignment of E-E results in a better score than E-D at the 7th position in the loop sequence. The remaining three cases have T-T favored over T-S, T-N and T-F at variable positions. The same comparison for all other error cases in this category is listed in Table 5. In particular, PAM30 substitution scores give an enormous favor to GG score compared to GD/GE/GA for cases coming from H1-13. Similarly, EE score is favored over ED, TT over TS/TN/TF in H2-8, II over IS/IT/IR, WW over WA/WS, YY over YS/YN in H2-10.\textbf{}

The categorization of misclassifications is also assigned to be either ``sufficiently good'' or ``to be improved'' categories shown in Table 1. There are several factors for the categorization of the misclassifications. For the first factor, the good misclassifications generally have greater between-cluster-exemplars dihedral-angle-distances compared to the bad misclassifications. (Table 2) The exception found is the "H1-13-7 to H1-13-1", which has relatively small between-cluster-exemplar-dihedral-angle distance but is one of the most improved misclassifications in the loop H1-13. For the second set of factors, out of all the clusters examined, misclassification cases from other clusters to H1-13-1 are enriched in both larger query-to-cluster-exemplar-dihedral-angle-distance spectrum and also the smaller neighbor-structure-number spectrum(Figure 5). These phenomena are not evident in the remaining examined clusters. These findings suggest that the two properties query-to-cluster-exemplar-distance and neighborhood-structure-count affect the chance of correct cluster identification in query structures from H1-13-1. We suggest that because H1-13-1 is more populated, the relative positions of CDRs w.r.t. the other CDRs in the H1-13-1 more frequently affects the chance of correct cluster identification than the other two clusters studied. The other clusters are less populated, the chance of misclassification is affected by the between-cluster-exemplar-dihedral-angle-distance to a greater extent than distances factors in the same cluster. 

GBM model for each loop and length type was constructed as described in the methods section specifies. The hyper-parameter tuning plot (Figure 6) shows that the model prediction accuracy plateaued once a specific model complexity was reached. When comparing GBM to blindBLAST. GBM model can improve accuracies for some loops but not others. We divide the loop and length types into two groups: ``better than blindBLAST'' or ``worse than blindBLAST'' based on the model accuracy and model stability. The ``worse than blindBLAST'' group can be further divided into two subtypes. The first has loops with greater assignment accuracies using GBM with complexity tuning than using blindBLAST but with high variance in model accuracy. Loops L1-13, L1-14, L1-15 and L3-9 have standard deviations from 5\% to 20\% in accuracy. These large deviations are likely related to the sparsity of the data. High standard deviation does belie the utility of these models. For example,the GBM model estimated accuracy for L1-15 is 10\% greater than that of blindBLAST, but with high standard deviation. The other subtype in this group does not achieve greater assignment accuracy than that of blind BLAST even with model tuning, such as H2-9. The remaining loops all fall in in the "better than blindBLAST" group. H1-13, H2-10, L3-9, L1-16 all achieve greater accuracy compared to blindBLAST with standard deviations of model accuracies below 5\%. H2-10 has the greatest improvement at 8\%. H1-13 takes longer iteration to plateau but achieves about 2\% higher accuracy than that of the blindBLAST . 

The reduction in classification error count by GBM in a loop and length type can be divided into different error-reductions each corresponding to a cluster-membership misclassification. For example, the large error count reduction in the GBM model of H2-10 (Figure 7(a)) can be divided into error count reductions corresponding to multiple misclassifications observed in H2-10(Figure 7(b)), with certain misclassifications such as "H2-10-1 to H2-10-6", "H2-10-2 to H2-10-6" having the greatest error count reductions. By examining the sequence composition between the cluster pair in these misclassifications, the improvements are not always achievable by simple sequence rules(Figure 7(b)). 

GBM model of H1-13 shows discernible improvement with 15 fewer error cases compared to 166.3 error count in blindBLAST, particularly shows better recoveries in cluster H1-13-1, H1-13-3, H1-13-4, H1-13-5(Figure 12). Such reductions in recoveries come from many small improvements in many misclassifications. There are other less noticeable improvement from GBM models in L1-11 and "L2-8-2 to L2-8-1" misclassification.

For the loops with GBM models achieving greater accuracy than the estimated prediction accuracies using blindBLAST such as L3-9, H2-10, L3-10, we extracted the variable importance from the best model Figure 10. The ranked feature importance suggested that the accuracy improvements come from the combinations of many features each weighed differently in the classification algorithm, and not replaceable by simple sequence rules. 
    
\section{Discussion}




\subsection{Blind BLAST accuracy of cluster prediction} In the 3-repeat-10-fold-cross-validation of the blindBLAST method, accuracies vary among the different CDR loop and length with many below 0.9. Figure 3 shows there are substantial increases in query-versus-template-structure RMSD using blindBLAST versus if the queries are aligned with the most similar CDR template in their clusters. The incorrect cluster-assignment observed using blindBLAST lead to a worse query-versus-template-structure RMSD, and the most substantial ones are in the H1-13, H2-10, L1-11 L1-12, L2-8, L3-9. Therefore improving the accuracy of cluster-assignment can improve the quality of template structure.

\begin{itemize}
  \item \relax 
\end{itemize}
  



\subsection{Blind BLAST has spectacular (worse than random) failures.}BlindBLAST cluster-assignment-accuracies are low for several loop and length types. We identified a few trends that connect the low accuracies with the properties of the clusters in the loop and length types. We find loop and length types with larger clusters have low cluster membership accuracy using blindBLAST compared to those with smaller cluster numbers. For example H1-13 has 8 clusters and below 80\% assignment accuracy whereas H1-15 and H2-9 have over 90\% assignment accuracy using BlindBLAST. L1-11 have lower accuracy than H1-13 and H2-10 for the same reason. In addition to the number of clusters, loop and length types with more balanced cluster-size-distributions have low accuracy. For example, H2-10 and H1-13 have a similar number of clusters, but H2-10 has two clusters H2-10-1 and H2-10-2 with more balanced member sizes than those of H1-13-1 and H1-13-2, resulting in low accuracy at 75\% compared to 78\% H1-13. L1-11 have smaller cluster number than L3-9, also has lower accuracy than L3-9 because of its greater cluster-member-sizes balance. Other than the total cluster number and cluster-member-size balance, loop and length types with small total sample size can have low accuracy.  Loops L1-12, L3-8, L3-10, L3-12 have the worst accuracies among all loops and dthey have both sparse sample sizes and greater cluster-member-size-balance. \textbf{}

We can intuitively understand there is a greater chance for misclassification when a loop and length type has a larger number of clusters. However, the lower accuracies in loop and length types with greater cluster-member-size balance indicates that more members in non-dominant clusters worsens blindBLAST searching accuracy. Finally, the lowest accuracies are for loop and length types with small sample sizes, suggesting that the accuracy can be limited by sparse data.  Next, we applied a statistical test (equation 3) to compare blindBLAST to the random assignment. A summary of misclassifications categorized by the significance is shown in Table 4. 

The misclassifications are divided into 4 categories as described in Table 1. Within each category except for the ``significantly worse than random'' group, misclassifications usually appear in pairs of involved cluster being the same but the order switched. Additionally, misclassifications in a category are concentrated to just a few loop and length types (Table 2). 

BLAST with PAM30 for short sequences is an established tool for homology sequence searching. However in some loop and length types, blindBLAST identifies templates with significantly higher error count than assigning clusters randomly. The cause of these lower than random-assignment performance is of our specific interest. As template selection are based on substitution scores, we examined their problematic substitution scores. The score at each loop position for the alignment to the wrong cluster was compared to the alignment to the correct cluster, where the correct alignment was obtained by template search only within the query cluster. We found five out of eight misclassification cases from L2-8-1 to L2-8-5 happened because the alignment of E-E results in a better score than E-D at the 7th position in the loop sequence. The remaining three cases have T-T favored over T-S, T-N and T-F at variable positions. The same comparison for all other error cases in this category is listed in Table 5. In particular, PAM30 substitution scores give an enormous favor to GG score compared to GD/GE/GA for cases coming from H1-13. Similarly, EE score is favored over ED, TT over TS/TN/TF in H2-8, II over IS/IT/IR, WW over WA/WS, YY over YS/YN in H2-10.\textbf{}

The categorization of misclassifications is also assigned to be either ``sufficiently good'' or ``to be improved'' categories shown in Table 1. There are several factors for the categorization of the misclassifications. For the first factor, the good misclassifications generally have greater between-cluster-exemplars dihedral-angle-distances compared to the bad misclassifications. (Table 2) The exception found is the "H1-13-7 to H1-13-1", which has relatively small between-cluster-exemplar-dihedral-angle distance but is one of the most improved misclassifications in the loop H1-13. For the second set of factors, out of all the clusters examined, misclassification cases from other clusters to H1-13-1 are enriched in both larger query-to-cluster-exemplar-dihedral-angle-distance spectrum and also the smaller neighbor-structure-number spectrum(Figure 5). These phenomena are not evident in the remaining examined clusters. These findings suggest that the two properties query-to-cluster-exemplar-distance and neighborhood-structure-count affect the chance of correct cluster identification in query structures from H1-13-1. We suggest that because H1-13-1 is more populated, the relative positions of CDRs w.r.t. the other CDRs in the H1-13-1 more frequently affects the chance of correct cluster identification than the other two clusters studied. The other clusters are less populated, the chance of misclassification is affected by the between-cluster-exemplar-dihedral-angle-distance to a greater extent than distances factors in the same cluster. 

GBM model for each loop and length type was constructed as described in the methods section specifies. The hyper-parameter tuning plot (Figure 6) shows that the model prediction accuracy plateaued once a specific model complexity was reached. When comparing GBM to blindBLAST. GBM model can improve accuracies for some loops but not others. We divide the loop and length types into two groups: ``better than blindBLAST'' or ``worse than blindBLAST'' based on the model accuracy and model stability. The ``worse than blindBLAST'' group can be further divided into two subtypes. The first has loops with greater assignment accuracies using GBM with complexity tuning than using blindBLAST but with high variance in model accuracy. Loops L1-13, L1-14, L1-15 and L3-9 have standard deviations from 5\% to 20\% in accuracy. These large deviations are likely related to the sparsity of the data. High standard deviation does belie the utility of these models. For example,the GBM model estimated accuracy for L1-15 is 10\% greater than that of blindBLAST, but with high standard deviation. The other subtype in this group does not achieve greater assignment accuracy than that of blind BLAST even with model tuning, such as H2-9. The remaining loops all fall in in the "better than blindBLAST" group. H1-13, H2-10, L3-9, L1-16 all achieve greater accuracy compared to blindBLAST with standard deviations of model accuracies below 5\%. H2-10 has the greatest improvement at 8\%. H1-13 takes longer iteration to plateau but achieves about 2\% higher accuracy than that of the blindBLAST . 

The reduction in classification error count by GBM in a loop and length type can be divided into different error-reductions each corresponding to a cluster-membership misclassification. For example, the large error count reduction in the GBM model of H2-10 (Figure 7(a)) can be divided into error count reductions corresponding to multiple misclassifications observed in H2-10(Figure 7(b)), with certain misclassifications such as "H2-10-1 to H2-10-6", "H2-10-2 to H2-10-6" having the greatest error count reductions. By examining the sequence composition between the cluster pair in these misclassifications, the improvements are not always achievable by simple sequence rules(Figure 7(b)). 

GBM model of H1-13 shows discernible improvement with 15 fewer error cases compared to 166.3 error count in blindBLAST, particularly shows better recoveries in cluster H1-13-1, H1-13-3, H1-13-4, H1-13-5(Figure 12). Such reductions in recoveries come from many small improvements in many misclassifications. There are other less noticeable improvement from GBM models in L1-11 and "L2-8-2 to L2-8-1" misclassification.

For the loops with GBM models achieving greater accuracy than the estimated prediction accuracies using blindBLAST such as L3-9, H2-10, L3-10, we extracted the variable importance from the best model Figure 10. The ranked feature importance suggested that the accuracy improvements come from the combinations of many features each weighed differently in the classification algorithm, and not replaceable by simple sequence rules. 


    
\section{Conclusions}




\subsection{Recap what we learned.}In our study, we have demonstrated that a CDR template from the corresponding structural cluster generally has lower RMSD than a template from the wrong cluster. We have examined the ability of RosettaAntibody to identify non-H3 CDR loop clusters implicitly using the PyIgClassify database. We trained GBM individually for each CDR loop and length type with multiple structural clusters and improved the canonical structural cluster identification accuracy from 81\% of RosettaAntibody to 85\% of GBM, for involved non-H3 loop and length type cumulatively. From the evaluation of blindBLAST cluster identification, some cluster pairs have shown to be better distinguished than other cluster pairs using blindBLAST method, while some cluster pairs have error case numbers not different from randomly assignment the clusters. In comparison between the blindBLAST result and the GBM results, those GBM introduced improvement come from the cluster pairs that are not successfully distinguished in the blindBLAST method. We find the bottleneck to further improvement are both the member size imbalance between clusters and data sparsity. Another factor limiting prediction accuracy is the between-cluster-exemplar dihedral angle distances, for example cluster pair L2-8-1 and L2-8-2 has a small distance and its GBM model cannot improve on the blindBLAST result. Finally, blindBLAST results show that cluster pairs with greater distances generally are better distinguished. In our study, discriminative power between clusters depends on distinct residue features, while cluster size balance, between cluster exemplar distances. 



\subsection{Future directions}As the bottleneck for CDR cluster prediction accuracy comes from the unbalanced data distribution, data sparsity, between-cluster-exemplar distances and the lack of distinctive sequence feature. Methods that can generate valid data to enrich clusters with sparse data can possibly improve the model estimation accuracy for the GBM model. A set of structures can be generated using RosettaDesign that lie in the cluster radius constraint to emulate the SMOTE method(Chawla et al., 2002) for enriching samples in unpopular classes. Another approach that leads to the same goal of increasing the member sizes of the unpopular clusters is to use semi unsupervised learning to incorporate the sequenced antibodies without solved structures. To address the fact that different between cluster exemplar distances is not reflected in the cluster identification accuracy nor in the training process, differential penalties for different misclassification can be applied in the machine learning model training process and used in the identification accuracy evaluate as well.

On the other hand, another tweak in the sampling and learning process can be done besides generating synthetic data for sparse clusters, such as training each weak learner with under-sampled dominant cluster rather than oversampling the non-dominant clusters. This method is stated to be more promising because of the more independent realization of individual decision trees. 
\section*{Acknowledgements}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{peerj-harv} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

    

\bibliographystyle{unsrt}

\bibliography{\jobname}

\end{document}    
  