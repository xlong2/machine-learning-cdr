# this script is a refactor of the Untitled.R in the R/ directory
# This script is a jazz version of the blind_blast.R in R/directory
# Purpose:
#    Read in a speficic loop type 
#    Do LOOCV for blind blast
#    Refer to the rmsd_table to get the rmsd from the template
#    Output the prediction result 
#    Do a blast search within the predicted cluster; Or a blast search outside of the clusters or globally if it is not identified as any cluster. 
#    Retrieve the rmsd between the predicted sequence and query sequence. Record it 
#    Also this script should enable blind blast search as currently employed in the rosetta
#    It should output the data format so that comparison between the prediction accuracy and rmsds are easy

# output:
#   File1:    Confusion table with rownames be the real cases and colnames be the prediction cases of all the predictions curated from LOOCV
#             rmsd_cluster_guided_blast_rmsd_",loop_type,"_",paste(c(each_method,cluster_dis),collapse="-"),"_conftable.rds"
#   File2:    Mean of the three best templated for each sequence and stored as a table  :
#               seq
#                H1_13-1.4312    1.003    1.0023  2.323  3.22
#                ...
#    
# Input:
#   
#   loop_type : "H2_9"
#   the_method  "blindblast"
#   cluster_dis :   "north"  / "rmsd"



library("caret")
#library(RSQLite)
#library("DBI")
#library("pryr")
#library("protr")
library("gbm")

test=TRUE
if(test){
  the_method="blindblast_just_get_alignment"
  cluster_dis="north"


  data_by_loop_type_list_unduplicated=readRDS("/Users/xlong3/lab_work_data/machine_learning_cdr/proline_classifier/data_by_loop_type_list_unduplicated_no_filtering.rds")

}else{
args <- commandArgs(trailingOnly = TRUE)
loop_type = args[1]
the_method = args[2]
cluster_dis = args[3]
rmsd_table_rdata_dir = "/home/xlong/pairwise_rmsd/rmsd_result/" # post all the L3_9_rmsd_table.Rdata kind of dta
data_by_loop_file_dir ="/home/xlong/machine_learning_cdr/Machine_learning_jazz_version/" 
load(paste(c(data_by_loop_file_dir,"data_by_loop_type_list_unduplicated.Rdata"),collapse=""))

}



subsitution_matrix_name ="wahtw"  #"/Volumes/lab/macbook/lab_work_data/vall_rmsd/loop_sub_matrix.csv"
subsitution_matrix="PAM30"
result_dir = "/Users/xlong3/lab_work_data/machine_learning_cdr/proline_classifier/"
mkcommand=paste(c("mkdir ",result_dir), collapse=" ")
system(mkcommand)
methods= c(the_method) # specify the machine learning methods to be used 
each_method=the_method

save_file = paste(c(result_dir,"blind_blast_cv_result_summary",".rds"),collapse="")
save_file1="/Users/xlong3/lab_work_data/machine_learning_cdr/proline_classifier/overall_accuracy.rds"
save_file2 = paste(c(result_dir,"10foldcvblast",".rds"),collapse="")
save_file3 = paste(c(result_dir,"all_pred_tables_realcluster_list",".rds"),collapse="")

make_reference_database <- function(member_seqs, features,fold_num) {
  #make a blast database
  seq_vec = apply(member_seqs[, features] , 1 , paste , collapse = "")
  names = member_seqs$identifier   # if the clustering scheme is by the torsion angles 
  cluster_types = member_seqs$cluster_type
  file = paste(c(loop_type,fold_num,"some_random_file_wont_check_again.fasta"),collapse = "")
  file.remove(file)
  for (index in 1:length(names)) {
    write(paste(">", names[index], fold_num,collapse = ""), file, append = TRUE)
    write(seq_vec[[index]], file, append = TRUE)
  }
  db_name = paste(c("some_db",loop_type,the_method,cluster_dis,fold_num),collapse="_")
  arg = paste(c(' -in ', file, ' -out ', db_name, ' -dbtype prot -hash_index '),
              collapse = " ")
  command = paste("makeblastdb ", arg)
  system(command)
  return(list(db_name))
}

runblast_and_retrive_rmsd <-function(returned_db, seq, features,fold_num){
  blastdbase_name = returned_db[[1]]  #the blastdbase is the directory of the database
  out_file = paste(c("./blast_his_file_need_to_parse_",loop_type,the_method,cluster_dis,fold_num,".txt"),collapse = "")  # the file to record hits result
  testing_seq_vec = apply(seq[, features] , 1 , paste , collapse = "")  # turn the query dataframe into strings
  file = paste(c("./testing_file_wont_check_again_",loop_type,the_method,cluster_dis,fold_num,".txt"),collapse=""); file.remove(file)
  write(paste(">", seq$identifier, collapse = ""), file, append =TRUE)
  write(testing_seq_vec[[1]], file, append = TRUE)      #write the sequence of the predicted
  if (grepl("PAM", subsitution_matrix)|grepl("BLO", subsitution_matrix)) {
    command = paste(
      c(
        'blastp' ,
        " -db ",
        blastdbase_name,
        " -query ",
        file ,
        " -out" ,
        out_file ,
        "-outfmt 6 -word_size 2  -max_target_seqs 1024 -num_threads 4 ",
        '  -evalue 2000 -matrix ',
        subsitution_matrix
      ),
      collapse = " "
    )
  }
  system(command)
  hit=2
  tryCatch({
    if(hit==2){
      command = paste(
        c(
          'blastp' ,
          " -db ",
          blastdbase_name,
          " -query ",
          file ,
          " -out" ,
          out_file ,
          "-outfmt 6 -word_size 2  -max_target_seqs 1024 -num_threads 4 ",
          '  -evalue 2000 -matrix ',
          "BLOSUM62"
        ),
        collapse = " "
      )
      system(command)
      hit = read.table(out_file, stringsAsFactors = FALSE)  #this table contains all the result of sequences and and the sequence corresponded hits
      
    }
    rmsds=c(NA,NA,NA)
    cluster = NA
    cluster = strsplit(hit[1,2],"\\.")[[1]][1];
    
    for(each_hit_ind in 1:3){    
      tryCatch({
        rmsds[each_hit_ind]=NA
        #print(c(hit[each_hit_ind,1],hit[each_hit_ind,2],rmsd_table[hit[each_hit_ind,1],hit[each_hit_ind,2]]))
        #rmsds[each_hit_ind]=rmsd_table[hit[each_hit_ind,1],hit[each_hit_ind,2]]
        }, error=function(e){})
    } # finish finding the hits rmsd
    rmsd_summary = c(rmsds,mean(rmsds,na.rm=TRUE),hit[1,2])
    return(rmsd_summary)
  }, error=function(e){})
  rmsd_summary = c( NA, NA,NA,NA,NA ) # in case of no alignment
  return(rmsd_summary)
}

make_3_10_cross_val<-function(training_cases,r,k){
  
  all_unique_ids=lapply(split(training_cases,training_cases$cluster_type),function(x){unique(x$id)})
  
  split_data=split(training_cases,training_cases$cluster_type)
  new_training_data=do.call(rbind,lapply(names(split_data),function(x){
    for(repeat_n in 1:r){
      all_unique_id_sample=lapply(all_unique_ids,function(y){if(length(y)>=k){sample(1:k,size=length(y),replace=TRUE)}else{sample(1:k,size=length(y),replace=FALSE)}})
      
      for(ind in 1:length(all_unique_ids[[x]])){
        all_unique_ids[[x]]
        repnum=dim(split_data[[x]][split_data[[x]]$id==all_unique_ids[[x]][ind],])[1]
        split_data[[x]][split_data[[x]]$id==all_unique_ids[[x]][ind],paste(c("fold.num",repeat_n),collapse="")]=rep(all_unique_id_sample[[x]][ind],repnum)
      }
    }
    return(split_data[[x]])
  }))
  
  
  # Create folds and repeats here - you could create your own if you want #
  
  
  folds.list.out <- list()
  folds.list <- list()
  list.counter <- 1
  for (y in 1:r) {
    newcol <- paste('fold.num', y, sep='')
    for (z in 1:k) {
      out_rown= which(new_training_data[,newcol]==z)  # find the folds in 
      folds_in =which(new_training_data[,newcol]!=z)
      a=new_training_data[folds_in,"id"]%in%  new_training_data[out_rown,"id"]
      print(a[a])
      
      sub=new_training_data[out_rown,"id"]  # ids corresponding to the fold
      out_rown=out_rown[which(!duplicated(sub))]
      a=new_training_data[folds_in,"id"]%in%  new_training_data[out_rown,"id"]
      print(a[a])
      if(length(a[a])!=0){
        print("The 1fold in and 9 fold out is not correct, check!")
        stop()
      }
      folds.list.out[[list.counter]] <- out_rown
      folds.list[[list.counter]] <- which(new_training_data[,newcol]!=z)
      list.counter <- list.counter + 1
    }
  }
  index_list=list()
  index_list[[1]]=folds.list.out
  index_list[[2]]=folds.list
  returned_list=list()
  returned_list[[1]]=index_list;
  returned_list[[2]]=new_training_data
  return(returned_list)
}

get_accuracy_per_fold<-function(each_fold){
  
  case_ids= sequences[folds.list.out[[each_fold]],"identifier"]
  fold_out_cases = sequences[folds.list.out[[each_fold]],]
  rmsd_list[[each_fold]]=list()
  for(each_ind in 1:length(case_ids)){
    #tryCatch({
    seq=fold_out_cases[each_ind,]
    case_id = seq["identifier"]
    member_seqs = sequences[folds.list[[each_fold]],]
    returned_db = make_reference_database(member_seqs, features,each_fold) #construct a blast database with the predicted cluster and find the best three hits , extract their rmsd
    rmsd_list[[each_fold]][[as.character(case_id)]] = runblast_and_retrive_rmsd(returned_db, seq,features,each_fold)
    returned_db=""
  }# end of iterating through the fold out cases for a single fold 

  # get the accuracy and accuracy SD for all the prediction accuracy. 
  rmsd_search_correct_cluster[[each_fold]]=list()
  rmsds_all = do.call(rbind, rmsd_list[[each_fold]])
  tbl=as.data.frame(rmsds_all)
  tbl[,4]=sapply(strsplit(as.character(rownames(tbl)),"\\."),"[[",1); tbl[,5]=as.character(tbl[,5])
  pred_clusters=sapply(strsplit(as.character(tbl[,5]),"\\."),"[[",1)
  matched=nrow(tbl[tbl[,4]==pred_clusters,])
  total = nrow(tbl)
  #names= unique(c(as.character(tbl[,1]),as.character(tbl[,2])))
  non_matched=which(tbl[,4]!=pred_clusters &!grepl("none",tbl[,4]))
  if(length(non_matched)!=0){
    fold_out_identifier=rownames(tbl)
    for(non_matched_ind in non_matched){
      this_seq=fold_out_identifier[non_matched_ind]
      seq=fold_out_cases[fold_out_cases$identifier==this_seq,]
      case_id = seq["identifier"]
      true_cluster=strsplit(as.character(case_id),"\\.")[[1]][1]
      member_seqs = sequences[folds.list[[each_fold]],] # these correspond to folds in
      if(grepl("none",true_cluster)){ # if the predicted cluster is none cluster, then do not restrict the databse to just include none cluster. 
        member_seqs=member_seqs
      }else{
      member_seqs=member_seqs[member_seqs$cluster_type==true_cluster,] } # only search within its true cluster 
      returned_db = make_reference_database(member_seqs, features,each_fold) #construct a blast database with the predicted cluster and find the best three hits , extract their rmsd
      rmsd_search_correct_cluster[[each_fold]][[as.character(case_id)]] = runblast_and_retrive_rmsd(returned_db, seq,features,each_fold)
      returned_db=""
    }
    rmsd_search_correct_cluster_all=do.call(rbind, rmsd_search_correct_cluster[[each_fold]])
    correct_cluster_tbl=as.data.frame(rmsd_search_correct_cluster_all)
    correct_cluster_tbl[,4]=sapply(strsplit(as.character(rownames(correct_cluster_tbl)),"\\."),"[[",1); correct_cluster_tbl[,5]=as.character(correct_cluster_tbl[,5])
  }else{correct_cluster_tbl=""}
  #conf_summary_table[[each_fold]]= matched/total
  result_list=list()
  result_list[[1]] = matched/total
  result_list[[2]] = tbl
  result_list[[3]] = correct_cluster_tbl
  
  return(result_list)
} # end of iterating all folds

the_method="somerandommethod"
runblast_and_retrive_similarity <-function(returned_db, seq, features){
  blastdbase_name = returned_db[[1]]  #the blastdbase is the directory of the database
  out_file = paste(c("./blast_his_file_need_to_parse_",loop_type,the_method,cluster_dis,"all",".txt"),collapse = "")  # the file to record hits result
  testing_seq_vec = apply(seq[, features] , 1 , paste , collapse = "")  # turn the query dataframe into strings
  file = paste(c("./testing_file_wont_check_again_",loop_type,the_method,cluster_dis,"all",".txt"),collapse=""); file.remove(file)
  write(paste(">", seq$identifier, collapse = ""), file, append =TRUE)
  write(testing_seq_vec[[1]], file, append = TRUE)      #write the sequence of the predicted
  if (grepl("PAM", subsitution_matrix)|grepl("BLO", subsitution_matrix)) {
    command = paste(
      c(
        'blastp' ,
        " -db ",
        blastdbase_name,
        " -query ",
        file ,
        " -out" ,
        out_file ,
        "-outfmt 6 -word_size 2  -max_target_seqs 10000 -num_threads 4 ",
        '  -evalue 200000 -matrix ',
        subsitution_matrix
      ),
      collapse = " "
    )
  }
  system(command)
  hit = read.table(out_file, stringsAsFactors = FALSE)  #this table contains all the result of sequences and and the sequence corresponded hits
  if(dim(hit)[1]==0){
  tryCatch({
      command = paste(
        c(
          'blastp' ,
          " -db ",
          blastdbase_name,
          " -query ",
          file ,
          " -out" ,
          out_file ,
          "-outfmt 6 -word_size 2  -max_target_seqs 1024 -num_threads 4 ",
          '  -evalue 2000 -matrix ',
          "BLOSUM62"
        ),
        collapse = " "
      )
      system(command)
      hit = read.table(out_file, stringsAsFactors = FALSE)  #this table contains all the result of sequences and and the sequence corresponded hits

    }, error=function(e){})
  }else{
    print("do nothing")
  }
  return(hit)
}
make_database <- function(member_seqs, features) {
  #make a blast database
  seq_vec = apply(member_seqs[, features] , 1 , paste , collapse = "")
  names = member_seqs$identifier   # if the clustering scheme is by the torsion angles 
  cluster_types = member_seqs$cluster_type
  file = paste(c(loop_type,"all","some_random_file_wont_check_again.fasta"),collapse = "")
  file.remove(file)
  for (index in 1:length(names)) {
    write(paste(">", names[index], "all",collapse = ""), file, append = TRUE)
    write(seq_vec[[index]], file, append = TRUE)
  }
  db_name = paste(c("some_db",loop_type,the_method,cluster_dis,"all"),collapse="_")
  arg = paste(c(' -in ', file, ' -out ', db_name, ' -dbtype prot -hash_index '),
              collapse = " ")
  command = paste("makeblastdb ", arg)
  system(command)
  return(list(db_name))
}





load_old=FALSE
rerun=TRUE
if(load_old){
overall_accuracy=readRDS(save_file1)
all_pred_tables_list=readRDS(save_file2)
result_summary=readRDS( save_file) 
all_pred_tables_realcluster_list=readRDS(save_file3)
}else{
# use gbm to do LOOCV by iterating all seqs
  overall_accuracy = list()
  all_pred_tables_list=list()
  all_pred_tables_realcluster_list =list()
}







for(loop_type in names(data_by_loop_type_list_unduplicated)){

  tryCatch({
  sequences = data_by_loop_type_list_unduplicated[[loop_type]][[1]]#load the sequences from a loop length
  features = data_by_loop_type_list_unduplicated[[loop_type]][[4]]
  each_loop_length_data_feature_string = data_by_loop_type_list_unduplicated[[loop_type]][[2]]
  sequences$cluster_type = as.character(sequences$cluster_type)
  all_cases =  sequences[,c(features,"cluster_type","identifier","id")]
  #make folds and separates folds in and folds out 
  r <-3 # number of repeats
  k <- 10 # number of folds
  training_cases=sequences
  training_cases=sequences[,c(features,"cluster_type","id","identifier")]
  returned_results=make_3_10_cross_val(training_cases,r,k)
  folds_spec=returned_results[[1]]
  sequences=returned_results[[2]]   # the training cases would have its mo
  folds.list.out=folds_spec[[1]]
  folds.list=folds_spec[[2]]
  counted_folds=c(1:length(folds.list))[unlist(lapply(folds.list.out,length))!=0]
  sequences[folds.list[[1]],"id"] %in%sequences[folds.list.out[[1]],"id"]
  rmsd_list= list() # the list to store the hit for each fold
  rmsd_search_correct_cluster=list()
  #conf_summary_table=list()
  #for(each_fold in 1:length(folds.list)){ # iterate through all the sequences in this cluster to 

  
  
  conf_summary_table=lapply(all_result_list,function(x){x[[1]]})
  if(!is.na(all_pred_tables_list[[loop_type]])){
    all_pred_tables=all_pred_tables_list[[loop_type]]
  }else{
    all_result_list=mclapply(as.list(counted_folds),get_accuracy_per_fold,mc.cores=6)
    
  all_pred_tables=lapply(all_result_list,function(x){x[[2]]})}
  all_pred_hit_realcluster_tables=lapply(all_result_list,function(x){x[[3]]})
  
  # get mean and get SD for the accuracy accross all folds 
  the_mean = mean(unlist(conf_summary_table))
  the_sd = sd(unlist(conf_summary_table))
  overall_accuracy[[loop_type]]=c(the_mean,the_sd)
  all_pred_tables_list[[loop_type]]=all_pred_tables
  all_pred_tables_realcluster_list[[loop_type]]=all_pred_hit_realcluster_tables
  },error=function(e){})
  }# end of iterating all loop types 


summary_list=lapply(all_pred_tables_list,function(x){lapply(x,function(y){right=y[y[,4]==sapply(strsplit(y[,5],"\\."),"[[",1),]; dim(right)[1]/dim(y)[1] })})
summary_list_frame=lapply(names(summary_list),function(name){data.frame(accu=t(as.data.frame(summary_list[[name]])),loop=rep(name,length(summary_list[[name]])))})
summary_list_frame_rbind=do.call(rbind,summary_list_frame)
  saveRDS(all_pred_tables_list,file= save_file2) 
  saveRDS(all_pred_tables_realcluster_list,file=save_file3)
  saveRDS(overall_accuracy,file=save_file1)
  #for each loop find out which ones are not the real cluster and get the query and predicted pdbs 
  result_summary = as.data.frame(do.call(rbind, overall_accuracy));  rownames(result_summary)=names(overall_accuracy);  colnames(result_summary)=c("mean","sd");  print(result_summary) ;print(save_file); result_summary$loop_type=rownames(result_summary);saveRDS(result_summary,file= save_file) 
  
  
# get the blindblast 10 fold cv accuracies 
  ggplot(result_summary,aes(x=loop_type,y=mean,ymin=mean-sd/2, ymax=mean+sd/2)) +geom_errorbar( stat = "identity") + geom_point()+ggtitle("Blind-blast best hit template class membership accuracy") +theme(plot.title = element_text(hjust = 0.5))
  options(digits=2)
 
  summary_list_frame_rbind$loop=as.character(summary_list_frame_rbind$loop)
  summary_list_frame_rbind$loop_length=sapply(strsplit(summary_list_frame_rbind$loop,"_"),"[[",2)
  summary_list_frame_rbind$loop_type=sapply(strsplit(summary_list_frame_rbind$loop,"_"),"[[",1)

  ggplot(summary_list_frame_rbind,aes(x=loop_length,y=accu)) +geom_boxplot() +facet_grid(.~loop_type,scales = "free", space="free")+ggtitle("Blind-blast best hit template class membership accuracy") +
    theme(plot.title = element_text(size=20,hjust = 0.5),axis.text.x= element_text(size=10))+ylab("Accuracy")+xlab("Loop")
  file_name="/Users/xlong3/lab_work_data/machine_learning_cdr/proline_classifier/Blindblast_class_membership_accuracy.png"
  ggsave(file=file_name,width=300,height=250, units = c( "mm"))
  
  
# analyze wrong prediction most prevalent cases
  
  all_pred_tables_list= readRDS(save_file2) 
  all_pred_tables_realcluster_list=readRDS(save_file3)
  overall_accuracy=readRDS(save_file1)
  all_pred=lapply(all_pred_tables_list,function(x){do.call(rbind,x)})
  most_freq=lapply(all_pred,function(x){
    print(x)
    x[,5]=sapply(strsplit(x[,5],"\\."),"[[",1);x=x[x[,4]!=x[,5],]; 
  fr=as.data.frame(table(x[,4],x[,5]))
    if(dim(fr)[1]!=0){
  fr=fr[fr$Freq!=0,]
  fr$ratio=fr$Freq/sum(fr$Freq)
  fr=fr[order(unlist(fr[,"Freq"]),decreasing = TRUE),]}else{fr=NULL}
  return(fr)
  })
  
  
  # perform 10 fold 3 repeats random prediction simulation
  
  for( x in names(all_pred_tables_list)){
    print(x)
    
  }
  
  overall_prefix="/Users/xlong3/lab_work_data/machine_learning_cdr/proline_classifier"
  file=paste(c(overall_prefix,"/data_by_loop_type_list_unduplicated_no_filtering_original.rds"),collapse = "")
  data_by_loop_type_list_unduplicated=readRDS(file)
  # get the summary by category 
  simulated_prediction=list()
  num_it=10
  
  get_pairwise_sequence_simi<-function(sequences,features){
    
    case_ids= sequences[,"identifier"]
    similarity_matrix=matrix(nrow=dim(sequences)[1],ncol=dim(sequences)[1])
    colnames(similarity_matrix)=case_ids
    rownames(similarity_matrix)=case_ids
    
    for(each_ind in 1:length(case_ids)){
      tryCatch({
      member_seqs = sequences[-each_ind,]
      returned_db = make_database(member_seqs, features) #construct a blast database with the predicted cluster and find the best three hits , extract their rmsd
      seq=sequences[each_ind,]
      hit_table = runblast_and_retrive_similarity(returned_db, seq,features)
      for(x in 1:dim(hit_table)[1]){
        similarity_matrix[each_ind,colnames(similarity_matrix)==hit_table[x,2]]=hit_table[x,12]
      } # finish recording all the available similarity values captured
      returned_db=""
      },error=function(e){})
      print(each_ind/length(case_ids))
    }
    return(similarity_matrix)
  }
  
  data(AAPAM30)
  AAPAM30_index=1:20
  names(AAPAM30_index)=rownames(AAPAM30)
  get_pairwise_sequence_simi_self_cal<-function(loop_type){
    sequences = data_by_loop_type_list_unduplicated[[loop_type]][[1]]#load the sequences from a loop length
    features = data_by_loop_type_list_unduplicated[[loop_type]][[4]]
    case_ids= sequences[,"identifier"]
    similarity_matrix=matrix(nrow=dim(sequences)[1],ncol=dim(sequences)[1])
    colnames(similarity_matrix)=case_ids
    rownames(similarity_matrix)=case_ids
    
    
    for(each_ind in 1:(length(case_ids)-1)){
      print(each_ind)
      for(j in (each_ind+1):length(case_ids)){
        tryCatch({
        a_seq = sequences[each_ind,features]
        b_seq =sequences[j,features]
        seq=rbind(a_seq,b_seq)
        similarity=sum(unlist(lapply(seq,function(x){AAPAM30[AAPAM30_index[[x[1]]],AAPAM30_index[[x[2]]]]})))
          similarity_matrix[each_ind,j]=similarity
        },error=function(e){})
        }
      
    }
    return(similarity_matrix)
  }
  
  
  all_similarity_matrix=mclapply(all_loops,get_pairwise_sequence_simi_self_cal,mc.cores=3)
  names(all_similarity_matrix)=all_loops
  saveRDS(all_similarity_matrix,file="~/lab_work_data/proline_classifier/all_similarity_matrix.rds")
  # code to compute the significance and effect size 
  all_dist_matrix=readRDS(file="/Users/xlong3/lab_work_data/proline_classifier/all_dist_matrix.rds")
  all_dists=lapply(all_dist_matrix,function(x){max(unlist(x),na.rm=TRUE)-min(unlist(x),na.rm=TRUE)})
  all_neighborhood_dist=unlist(all_dists)/6
  lapply()
  
  
  #compute for each loop
  #all_similarity_matrix[[1]][1:5,1:5]
  all_significance_simulation=list()  # the list to record all the test count 
  for(each_l in names(all_similarity_matrix)){
    #get sequences 
    sequences = data_by_loop_type_list_unduplicated[[each_l]][[1]]#load the sequences from a loop length
    features = data_by_loop_type_list_unduplicated[[each_l]][[4]]
    case_ids= sequences[,"identifier"]
    # do 100 times sampling to get the distribution of the sd 
    splitted_sequence=split(sequences,sequences$cluster_type)
  clusters=unique(sequences$cluster_type)
  prediction_it=list()
  iteration_n=1000
  for(it in 1:iteration_n){
    print(c(each_l,it/iteration_n))
    all_clu_re=list()
  for(cluster in clusters){
  ind_cl=which(sequences$cluster_type==cluster)
  this_ind_most_sim_ind_list=list()
  for(this_ind in ind_cl){
  relevant_indexes=(1:dim(sequences)[1])[1:dim(sequences)[1]!=ind_cl]
  
  this_ind_most_sim_ind=sample(relevant_indexes,1,replace=TRUE)
  this_ind_most_sim_ind_list=c(this_ind_most_sim_ind_list,this_ind_most_sim_ind)
  }
    final_table=data.frame(sequences[ind_cl,"cluster_type"],sequences[unlist(this_ind_most_sim_ind_list),"cluster_type"])
    all_clu_re[[cluster]]=final_table
    
      }
    prediction_it[[it]]=as.data.frame(table(do.call(rbind,all_clu_re)))
    all_clu_re=""
    gc()
  }# end of 1000 iteration for this loop
  
  prediction_all=do.call(rbind, prediction_it)
  prediction_all$error_type=paste(prediction_all[,1],prediction_all[,2],sep="")
  all_the_values=split(prediction_all[,"Freq"],prediction_all[,"error_type"])
  prediction_it=""
  
  all_significance_simulation[[each_l]]=all_the_values
  all_the_values=""
  gc()
  }
  
  saveRDS(all_significance_simulation,file="~/lab_work_data/proline_classifier/all_significance_simulation.rds")
  
  # get the specific prediction using similarity scores 
  most_similar_score_result=list()
  all_loop_sig_frame_list=list()
  library(Matrix)
  for(each_l in names(all_similarity_matrix)){
    #get sequences 
    this_dis=all_dist_matrix[[each_l]]
    this_dis=forceSymmetric(this_dis)
    this_simi=all_similarity_matrix[[each_l]]
    this_simi=forceSymmetric(this_simi)
    sequences = data_by_loop_type_list_unduplicated[[each_l]][[1]]#load the sequences from a loop length
    if(dim(this_dis)[1]!=dim(sequences)[1]){
      print("number does not match stop!")
      stop()
    }
    features = data_by_loop_type_list_unduplicated[[each_l]][[4]]
    case_ids= sequences[,"identifier"]
    # do 100 times sampling to get the distribution of the sd 
   # splitted_sequence=split(sequences,sequences$cluster_type)
    clusters=unique(sequences$cluster_type)
    prediction_it=list()

      print(c(each_l))
      all_clu_re=list()
      most_similar_list=list()
      for(cluster in clusters){
        ind_cl=which(sequences$cluster_type==cluster)
        this_ind_most_sim_ind_list=list()
        for(this_ind in ind_cl){
         # print(this_ind)
          relevant_indexes=(1:dim(sequences)[1])[(1:dim(sequences)[1])!=this_ind]
          the_max=max(unlist(this_simi[this_ind,]),na.rm=TRUE)
          this_ind_most_sim_ind=which(unlist(this_simi[this_ind,])==the_max)[1]
         # if(length(unlist(this_ind_most_sim_ind))>1){
        #    stop()
        #  }
          print(as.data.frame(c(this_ind, this_ind_most_sim_ind)))
          this_ind_most_sim_ind_list=c(this_ind_most_sim_ind_list,this_ind_most_sim_ind)
        }
        final_table=data.frame(sequences[ind_cl,"cluster_type"],sequences[unlist(this_ind_most_sim_ind_list),"cluster_type"])
        match_result=data.frame(query=ind_cl,template=unlist(this_ind_most_sim_ind_list))
        tem=cbind(match_result,final_table)
        tem$out_of_cluster_similarity=rep(NA,dim(tem)[1])
        tem$within_cluster_similarity=rep(NA,dim(tem)[1])
        all_disin_query_cluster=sequences[ind_cl,"dis"]
        percentile <- ecdf(all_disin_query_cluster)
        
        for(x in 1:dim(tem)[1]){
          query_id=tem[x,"query"]
          template_id=tem[x,2]
          query_cluster=tem[x,3]
          template_cluster=tem[x,4]
          temcluster_ids=which(sequences$cluster_type==template_cluster)
          template_dis_s=sequences[temcluster_ids,"dis"]
          temp_percentile=ecdf(template_dis_s)
          non_querycluster_cluster=which(sequences$cluster_type!=query_cluster)
          print(c("query_id ", query_id))
          within_querycluster_ind=ind_cl[ind_cl!=query_id]
          within_most_sim=max(unlist(this_simi[query_id,within_querycluster_ind]),na.rm=TRUE)
          query_to_querycluster_dis=sequences[query_id,"dis"]
          #query_dis_percentile = percentile(query_to_querycluster_dis)
          template_to_tempcluster_dis=sequences[template_id,"dis"]
          query_dis_percentile=percentile(query_to_querycluster_dis)
          temp_dis_percentile=temp_percentile(template_to_tempcluster_dis)
          out_most_sim=max(unlist(this_simi[query_id,non_querycluster_cluster]),na.rm=TRUE)
          #number_nei=which()
          tem[x,"out_of_cluster_similarity"]=out_most_sim
          tem[x,"within_cluster_similarity"]=within_most_sim
          tem[x,"query_to_cluster_distance"]= query_to_querycluster_dis
          tem[x,"query_to_clustercen"]=query_dis_percentile
          tem[x,"template_to_clustercen"]=temp_dis_percentile
          tem[x,"number_neighborhood_structure"]=length(which(unlist(this_dis[tem[x,"query"],])<all_neighborhood_dist[[each_l]]))
          
        }
        most_similar_list[[cluster]]=tem
        all_clu_re[[cluster]]=final_table
        
      }# end of iterating all clusters in this loop type 
      prediction_it=as.data.frame(table(do.call(rbind,all_clu_re)))
      most_similar_score_result[[each_l]]=do.call(rbind,most_similar_list)
      all_clu_re=""
      gc()
      
      # get a p value and also an effect size 
    prediction_all=prediction_it
    prediction_all$error_type=paste(prediction_all[,1],prediction_all[,2],sep="")
    

    
    significance_list = list()
    effect_size_list=list()
    this_error_list=list()
    mean_retrieve_the_errors_from_random=list()
    sd_list=list()
    query_total_count=list()
    for (iter_error in 1:length(prediction_all$error_type) ){  # start iterating over the error types in this loop to assess its significance
      tryCatch({
      each_error = prediction_all$error_type[iter_error]
      retrieve_the_errors_from_random = all_significance_simulation[[each_l]][[each_error]]
      this_error = prediction_all[iter_error, "Freq"]
      aycdf <- ecdf(retrieve_the_errors_from_random)
      
      significance = aycdf(this_error)
      significance_list[[iter_error]] = significance
      this_error_list[[iter_error]]=this_error
      sd_list[[iter_error]]=sd(retrieve_the_errors_from_random)
      mean_retrieve_the_errors_from_random[[iter_error]]=mean(retrieve_the_errors_from_random)
      
      effect_size_list[[iter_error]]=(this_error-mean(retrieve_the_errors_from_random))/sd(retrieve_the_errors_from_random)
      },error=function(e){})
    }# end of iterating over all errors in this this loop type
    sig_data_frame = data.frame(error_type = prediction_all$error_type,
                                significance = unlist(significance_list),effect_size=unlist(effect_size_list),error_count=unlist(this_error_list),mean_simu_error=unlist(mean_retrieve_the_errors_from_random),sd=unlist(sd_list),Var1=prediction_all[,1],Var2=prediction_all[,2])
    sig_data_frame=sig_data_frame[sig_data_frame$mean_simu_error>1 | (sig_data_frame$significance>=0.975|sig_data_frame$significance<=0.025),]
    all_loop_sig_frame_list[[each_l]] = sig_data_frame
    
    sig_data_frame=""
    all_the_values = ""
    gc()
  }
  
  
  most_similar_score_result = lapply(most_similar_score_result,function(x){names(x)= c("query", "template","query_cluster","template_cluster","outcluster_similarity","incluster_similarity","query_to_cluster_distance","query_to_clustercen","template_to_clustercen")
   return(x)
  })
  
  save(most_similar_score_result,file="~/lab_work_data/proline_classifier/most_similar_score_result.rds")
  save(all_loop_sig_frame_list,file="~/lab_work_data/proline_classifier/all_loop_sig_frame_list.rds")
  
  
  # make a plot to show the most similar similarity scores within and out of the correct cluster
  all_list=list()
  H1_13_1=most_similar_score_result[["H1_13"]]
  all_list[["H1_13_1"]]=H1_13_1[H1_13_1$query_cluster=="H1-13-1",]
  H2_10_1=most_similar_score_result[["H2_10"]]
  all_list[["H2_10_1"]]=H2_10_1[H2_10_1$query_cluster=="H2-10-1",]
  L2_8_1=most_similar_score_result[["L2_8"]]
  all_list[["L2_8_1"]]=L2_8_1[L2_8_1$query_cluster=="L2-8-1",]
  L3_9_2=most_similar_score_result[["L3_9"]]
  all_list[["L3_9_2"]]=L3_9_2[L3_9_2$query_cluster=="L3-9-2",]
  all_lsit_frame=do.call(rbind,all_list)
  for(x in 1:dim(all_lsit_frame)[1]){

    loop_type=paste(strsplit(as.character(all_lsit_frame[x,"query_cluster"]),"-")[[1]][1:2],collapse="_")
    sequences = data_by_loop_type_list_unduplicated[[loop_type]][[1]]#load the sequences from a loop length
    
    query_center_id=which(sequences$center==1 &sequences$cluster_type==as.character(all_lsit_frame[x,"query_cluster"]))
    simi_frame=forceSymmetric( all_similarity_matrix[[loop_type]])
    all_lsit_frame[x,"query_to_center_similarity"]=simi_frame[all_lsit_frame[x,"query"],query_center_id]
  }
  all_lsit_frame[all_lsit_frame$template_cluster==all_lsit_frame$query_cluster,"cluster"]=rep("matched",length(which(all_lsit_frame$template_cluster==all_lsit_frame$query_cluster)))
  all_lsit_frame[all_lsit_frame$template_cluster!=all_lsit_frame$query_cluster,"cluster"]= as.character(all_lsit_frame[all_lsit_frame$template_cluster!=all_lsit_frame$query_cluster,"template_cluster"])
  
  all_lsit_frame[all_lsit_frame$query_cluster==all_lsit_frame$template_cluster,"matched_or_not"]=rep("matched",dim(all_lsit_frame[all_lsit_frame$query_cluster==all_lsit_frame$template_cluster,])[1])
  all_lsit_frame[all_lsit_frame$query_cluster!=all_lsit_frame$template_cluster,"matched_or_not"]=rep("unmatched",dim(all_lsit_frame[all_lsit_frame$query_cluster!=all_lsit_frame$template_cluster,])[1])
  
  all_lsit_frame$type=paste(all_lsit_frame$query_cluster,all_lsit_frame$matched_or_not,sep="")
  all_lsit_frame_a=all_lsit_frame; all_lsit_frame_b=all_lsit_frame; all_lsit_frame_c=all_lsit_frame
  all_lsit_frame_c$matched_or_not=rep("all",dim(all_lsit_frame_c)[1])
  all_lsit_frame_a$value=all_lsit_frame$query_to_clustercen
  all_lsit_frame_b$value=all_lsit_frame$query_to_clustercen
  all_lsit_frame_c$matched_or_not=rep("all",dim(all_lsit_frame_c)[1])
  all_lsit_frame_c$value=all_lsit_frame_c$query_to_clustercen
  
  all_lsit_frame_c$query_to_center_similarity=-all_lsit_frame_c$query_to_center_similarity
  ggplot(all_lsit_frame_c)+geom_jitter(aes(x=query_to_center_similarity,y=query_to_cluster_distance),alpha=0.5)+facet_wrap(~query_cluster,scales="free")+
    ggtitle("corrleation of sequence identity to cluster ")+xlab("negative sequence similarity score")
  ggsave("~/lab_work_data/proline_classifier/all_lsit_frame_corrleation.pdf", plot = last_plot(), device = NULL, path = NULL,
         width = 7.5, height = 5, units = "in", limitsize = TRUE)
  
  
  
  all_lsit_frame_ab=rbind(all_lsit_frame_a,all_lsit_frame_c)
  
  ggplot(all_lsit_frame)+geom_bar(aes(x=query_to_clustercen))+facet_wrap(~type,scales="free")
  ggplot(all_lsit_frame_ab)+geom_density(aes(x=value,fill=matched_or_not),alpha=0.5)+facet_wrap(~query_cluster,scales="free")+
    ggtitle("To cluster center distance of matched and unmatched cdrs")+xlab("dihedral distance quantile")+theme()
  ggsave("~/lab_work_data/proline_classifier/all_lsit_frame_ab.pdf", plot = last_plot(), device = NULL, path = NULL,
         width = 7.5, height = 5, units = "in", limitsize = TRUE)
  
  
  ggplot(all_lsit_frame)+geom_density(aes(x=template_to_clustercen))+facet_wrap(~type,scales="free")
  
  
  
  all_lsit_frame$query_to_clustercen=as.numeric(all_lsit_frame$query_to_clustercen)
  plot(density(all_lsit_frame[all_lsit_frame$query_cluster=="H1-13-1","query_to_clustercen"]))
  plot(density(all_lsit_frame$query_to_clustercen))
    ggplot(all_lsit_frame)+geom_point(alpha=0.7,aes(x=outcluster_similarity,y=incluster_similarity,color=cluster))+
  
          facet_wrap(~query_cluster,scales="free")+geom_abline()+ theme_classic() #+

    
    
          #scale_fill_manual("cluster", values = col_vector)
    ggsave(paste(c(plot_dir,"all_lsit_frame.pdf"),collapse=""), plot = last_plot(), device = NULL, path = NULL,
           width = 7.5, height = 5, units = "in", limitsize = TRUE)
   
    all_lsit_frame_H1_13= all_lsit_frame[grepl("H1-13",all_lsit_frame[,"type"]),]  

    
    library("grid")
      png("test.png")
    
    #A viewport taking up a fraction of the plot area
    vp <- viewport(width = 0.2, height = 0.4, x = 0.8, y = 0.2)
    plot_a=ggplot(all_lsit_frame_H1_13)+geom_point(alpha=0.7,aes(x=outcluster_similarity,y=incluster_similarity,color=cluster))+
      facet_wrap(~query_cluster,scales="free")+geom_abline()+ ylab("best in cluster similarity")+ xlab("best out of cluster similarity")+theme_classic() +theme(axis.text=element_text(size=9),
    axis.title=element_text(size=9,face="bold"))
    ggsave("~/plot_a.pdf", plot = last_plot(), device = NULL, path = NULL,
           width = 5, height = 3.5, units = "in", limitsize = TRUE)
    
   # small_plot =
      ggplot(data_frame_r_H1_13)+ggtitle("The right cluster vs wrong cluster template query rmsd")+theme(plot.title = element_text(hjust = 0.5))+
      ylab("query vs template rmsd")+ylim(0,8)+facet_grid(.~loop_type,scales = "free", space="free")+
      geom_boxplot(aes(x=loop,y=V1,color=annotate),alpha=0.3,inherit.aes = FALSE)+ theme_classic()+theme(axis.text=element_text(size=9),
                                                                                                         axis.title=element_text(size=9,face="bold"),legend.text=element_text(size=9), legend.position="bottom")+
        guides(color=guide_legend(title=""))+xlab("")
    ggsave("~/small_plot.pdf", plot = last_plot(), device = NULL, path = NULL,
           width = 1, height = 2.5, units = "in", limitsize =FALSE )
    
    
    #Just draw the plot twice
    print(plot_a)
    print(small_plot, vp = vp)
    dev.off()
    
    
    
    
    
    
     
    apply(all_lsit_frame,1,function(x){
      num=as.numeric(x["query"])
      cluster=num()
      sequences()
      })
    
    
    library(RColorBrewer)
    n <- length(unique(all_lsit_frame$cluster))+1
    qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
    col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
    #col_vector=col_vector[c(1:3,5:n)]

  # make the plot to show which ones are the most important 
  all_error_sig_info=do.call(rbind,lapply(names(all_loop_sig_frame_list),function(y){
    x=all_loop_sig_frame_list[[y]];x$loop=rep(y,dim(x)[1])
    # if the first column has the cluster one and the two clusters are not the ssame
    H1_related=x[x[,"Var1"]!=x[,"Var2"],]
    # most prevalent cluster 
    each_l=paste(unlist(strsplit(y,"_")[[1]]),collapse="-")
    if(each_l=="L3-9"){
      most_p="L3-9-cis7-1$"
    }else{
      most_p=paste(c(each_l,"-1$"),collapse="")
    }
    H1_cluster_one_recovery_fail=H1_related[grepl(most_p,H1_related$Var1),]
    H1_cluster_one_precision_fail=H1_related[grepl(most_p,H1_related$Var2),]
    H1_cluster_one_recovery_fail$error_type=rep("1_reco",dim(H1_cluster_one_recovery_fail)[1])
    H1_cluster_one_precision_fail$error_type=rep("1_prec",dim(H1_cluster_one_precision_fail)[1])
    other_types=H1_related[!(grepl(most_p,H1_related$Var1) |grepl(most_p,H1_related$Var2)),]
    other_types$error_type=rep("non_1",dim(other_types)[1])
    all_annotated=rbind(rbind(other_types,H1_cluster_one_precision_fail),H1_cluster_one_recovery_fail)
    return(all_annotated)
    # if the 
    }))
  all_error_sig_info=all_error_sig_info[all_error_sig_info$Var1!=all_error_sig_info$Var2,]
  all_error_sig_info$sig_id=rep("insignificant",dim(all_error_sig_info)[1])
  all_error_sig_info[all_error_sig_info$significance<=0.025,"sig_id"]=rep("smaller",length(all_error_sig_info[all_error_sig_info$significance<=0.025,"sig_id"]))
  all_error_sig_info[all_error_sig_info$significance>=0.975,"sig_id"]=rep("larger",length(all_error_sig_info[all_error_sig_info$significance>=0.975,"sig_id"]))
  
  all_right_sig_info=do.call(rbind,lapply(names(all_loop_sig_frame_list),function(y){
    x=all_loop_sig_frame_list[[y]];x$loop=rep(y,dim(x)[1])
    # if the first column has the cluster one and the two clusters are not the ssame
    H1_related=x[x[,"Var1"]==x[,"Var2"],]
    # most prevalent cluster 
    each_l=paste(unlist(strsplit(y,"_")[[1]]),collapse="-")
    if(each_l=="L3-9"){
      most_p="L3-9-cis7-1$"
    }else{
      most_p=paste(c(each_l,"-1$"),collapse="")
    }
    H1_cluster_one_recovery_fail=H1_related[grepl(most_p,H1_related$Var1),]
    H1_cluster_one_recovery_fail$error_type=rep("cluster_1",dim(H1_cluster_one_recovery_fail)[1])
    other_types=H1_related[!(grepl(most_p,H1_related$Var1)) ,]
    other_types$error_type=rep("non_1",dim(other_types)[1])
    all_annotated=rbind(other_types,H1_cluster_one_recovery_fail)
    return(all_annotated)
    # if the 
  }))
  
  all_right_sig_info$sig_id=rep("insignificant",dim(all_right_sig_info)[1])
  all_right_sig_info[all_right_sig_info$significance<=0.025,"sig_id"]=rep("smaller",length(all_right_sig_info[all_right_sig_info$significance<=0.025,"sig_id"]))
  all_right_sig_info[all_right_sig_info$significance>=0.975,"sig_id"]=rep("larger",length(all_right_sig_info[all_right_sig_info$significance>=0.975,"sig_id"]))
 
    ggplot(all_right_sig_info)+geom_point(aes(x=error_type,y=effect_size,color=sig_id))+facet_wrap(~loop,scales="free_y")+
    ggtitle("The effect size of the right classification")+scale_color_manual(values=c("blue", "green", "red"))+theme(axis.text.x = element_text(size=9,angle=90, hjust=1))
  ggsave("~/lab_work_data/proline_classifier/all_right_sig_info.pdf", plot = last_plot(), device = NULL, path = NULL,
         width = 7.5, height = 5, units = "in", limitsize = TRUE)
  
  
  effect_size_large=all_error_sig_info[all_error_sig_info$effect_size>=2 &all_error_sig_info$sig_id=="larger",]
  effect_size_large=effect_size_large[order(effect_size_large$error_count,decreasing=TRUE),]
  effect_size_large_sig=effect_size_large[effect_size_large$error_count>3,]
  effect_size_large_sig # for this cluster 
  # for each of the pair # find out the specific sequences that generate worng cases with cluster Var1 predicted to cluster Var2
  most_similar_score_result_with_cluster=lapply(names(most_similar_score_result),function(x){
    y=most_similar_score_result[[x]]
    sequences = data_by_loop_type_list_unduplicated[[x]][[1]]#load the sequences from a loop length
    features = data_by_loop_type_list_unduplicated[[x]][[4]]
    y$query_cluster=as.character(sequences[y[,1],"cluster_type"])
    y$query_sequence=as.character(apply(sequences[y[,1],features],1,function(x){paste(unlist(x),collapse="")}))
    y$template_cluster=as.character(sequences[y[,2],"cluster_type"])
    y$templaste_sequence=as.character(apply(sequences[y[,2],features],1,function(x){paste(unlist(x),collapse="")}))
    
    return(y)
    
  })
  names(most_similar_score_result_with_cluster)=names(most_similar_score_result)
  wrong_info_list=list()
  specific_pairing_problems=list()
  for(each_wrong_cat in 1:dim(effect_size_large_sig)[1]){
    print(each_wrong_cat)
    Var1=as.character(effect_size_large_sig[each_wrong_cat,"Var1"])
    Var2=as.character(effect_size_large_sig[each_wrong_cat,"Var2"])
    most_similar_this_loop=most_similar_score_result_with_cluster[[effect_size_large_sig[each_wrong_cat,"loop"]]]
    wrong_info=most_similar_this_loop[most_similar_this_loop$query_cluster==Var1 & most_similar_this_loop$template_cluster==Var2,]
    wrong_info$wrong_cat=paste(wrong_info$query_cluster,wrong_info$template_cluster,sep="")

    result=apply(wrong_info,1,function(h){   
    query_seq=strsplit(as.character(h["query_sequence"]),"")[[1]]
    tem_seq=strsplit(as.character(h["templaste_sequence"]),"")[[1]]
    pos=which(query_seq!=tem_seq)
    a=as.character(query_seq[pos])
    b=as.character(tem_seq[pos])
    result=sapply(1:length(pos),function(x){
      paste(c(pos[x],sort(c(a[x],b[x]))),collapse="")
    })
    return(result)
    })
    specific_pairing_problems[[paste(c(Var1,Var2),collapse="")]]=result
    wrong_info_list[[paste(c(Var1,Var2),collapse="")]]=wrong_info
  }

  num_mis_match=do.call(rbind, lapply(names(specific_pairing_problems),function(y){x=specific_pairing_problems[[y]];k=as.data.frame(table(unlist(lapply(x,length)))); k$type=rep(y,dim(k)[1]);return(k)}))
  ggplot(num_mis_match)+geom_bar(aes(x=Var1,y=Freq),stat="identity")+facet_grid(~type)+ theme(strip.text.x = element_text(size = 8)) +xlab("mismatch a.a. number")+ggtitle("Mismatch a.a. frequency")
  ggsave("~/lab_work_data/proline_classifier/num_mis_match.pdf", plot = last_plot(), device = NULL, path = NULL,
         width = 7.5, height = 5, units = "in", limitsize = TRUE)
  
  
  
  not_much_improvement=all_error_sig_info[all_error_sig_info$error_count>3 &(all_error_sig_info$sig_id=="insignificant"),]
  not_much_improvement= not_much_improvement[order(not_much_improvement$error_count),]
  not_much_improvement_output=not_much_improvement[,c("error_count","mean_simu_error","sd","Var1","Var2","significance")]
  names(not_much_improvement_output)=c("error_count","mean_simu_error_count","sd","query_cluster","template_cluster","significance")
  write.csv(not_much_improvement_output,file="~/lab_work_data/not_much_improvement_output.csv")
  
  
  
  improvement_but_still_many_errors=all_error_sig_info[all_error_sig_info$error_count>3 &(all_error_sig_info$sig_id=="smaller"),]
  improvement_but_still_many_errors=improvement_but_still_many_errors[,c("error_count","mean_simu_error","sd","Var1","Var2","significance")]
  names(improvement_but_still_many_errors)=c("error_count","mean_simu_error_count","sd","query_cluster","template_cluster","significance")
  write.csv(improvement_but_still_many_errors,file="~/lab_work_data/improvement_but_still_many_errors.csv")
  
    improvement_and_not_very_much_error=all_error_sig_info[all_error_sig_info$error_count<=3 &(all_error_sig_info$sig_id=="smaller"),]
    improvement_and_not_very_much_error=improvement_and_not_very_much_error[,c("error_count","mean_simu_error","sd","Var1","Var2","significance")]
    names(improvement_and_not_very_much_error)=c("error_count","mean_simu_error_count","sd","query_cluster","template_cluster","significance")
    write.csv(improvement_and_not_very_much_error,file="~/lab_work_data/improvement_and_not_very_much_error.csv")
    
  
  
    wrong_paring_dissimilar_sequences=lapply(specific_pairing_problems,function(x){table(unlist(x))})
  lapply(specific_pairing_problems,function(x){table(unlist(x))})
  
  wrong_info_frame=do.call(rbind, lapply(wrong_info_list,function(x){print(x[,1]);data.frame(x$query_cluster[1],x$template_cluster[1],length(unique(x[,1])),length(unique(x[,2])), sort(table(x[,2]),decreasing=TRUE)[1])}))
  colnames(wrong_info_frame)=c("query_cluster","template_cluster","unique_query_cases","unique_template_cases","template_with_largest_error_number")
  write.csv(wrong_info_frame,file="~/lab_work_data/query_template.csv")
  
  
  ggplot(wrong_info_frame)
  
  
  ggplot(all_error_sig_info)+geom_point(aes(x=error_type,y=effect_size,color=sig_id))+facet_wrap(~loop,scales="free_y")+
    ggtitle("The effect size of the wrong classification")+scale_color_manual(values=c("blue", "red", "green"))+theme(axis.text.x = element_text(size=9,angle=90, hjust=1))
  ggsave("~/lab_work_data/proline_classifier/all_error_sig_info.pdf", plot = last_plot(), device = NULL, path = NULL,
         width = 7.5, height = 5, units = "in", limitsize = TRUE)
  
  
  all_loops_similarity_matrix_by_self=list()
  all_loops=names(data_by_loop_type_list_unduplicated)[order(unlist(lapply(data_by_loop_type_list_unduplicated,function(x){dim(x[[1]])[1]})),decreasing=TRUE)]
  all_similarity_matrix=mclapply(all_loops,get_pairwise_sequence_simi_self_cal,mc.cores=3)
    for(loop_type in names(data_by_loop_type_list_unduplicated)){

    bb=get_pairwise_sequence_simi_self_cal(sequences,features)
    all_loops_similarity_matrix_by_self[[loop_type]]=bb
    #saveRDS(bb,file=paste(c("all_loops_similarity_matrix",loop_type,".rds"),collapse=""))
    gc()
  }
    


  all_loops_similarity_matrix=list()
  for(loop_type in names(data_by_loop_type_list_unduplicated)){
    sequences = data_by_loop_type_list_unduplicated[[loop_type]][[1]]#load the sequences from a loop length
    features = data_by_loop_type_list_unduplicated[[loop_type]][[4]]
    bb=get_pairwise_sequence_simi(sequences,features)
    saveRDS(bb,file=paste(c("all_loops_similarity_matrix",loop_type,".rds"),collapse=""))
    gc()
  }
  
  
  for(loop_type in names(data_by_loop_type_list_unduplicated)){

    all_loops_similarity_matrix[[loop_type]]=readRDS(file=paste(c("all_loops_similarity_matrix",loop_type,".rds"),collapse=""))
    
  }
  saveRDS(all_loops_similarity_matrix,file="~/lab_work_data/proline_classifier/all_loops_similarity_matrix.rds")
  
  for(loop_type in names(data_by_loop_type_list_unduplicated)){
    
   # tryCatch({
      sequences = data_by_loop_type_list_unduplicated[[loop_type]][[1]]#load the sequences from a loop length
      features = data_by_loop_type_list_unduplicated[[loop_type]][[4]]
      each_loop_length_data_feature_string = data_by_loop_type_list_unduplicated[[loop_type]][[2]]
      sequences$cluster_type=as.character(sequences$cluster_type) 
      all_cases =  sequences[,c(features,"cluster_type","identifier","id")]
      #make folds and separates folds in and folds out 
      r <-3 # number of repeats
      k <- 10 # number of folds
      training_cases=sequences
      training_cases=sequences[,c(features,"cluster_type","id","identifier")]
      all_results=list()
      print(loop_type)
      for(sim_i in 1:num_it){
        print(sim_i)
      returned_results=make_3_10_cross_val(training_cases,r,k)
      divisions_out=returned_results[[1]][[1]]
      divisions_in=returned_results[[1]][[2]]
      this_iter_result=list()
      for(i in 1:30){
        divisions_out[[i]]
        divisions_in[[i]]
        result=sample(divisions_in[[i]],length(divisions_out[[i]]),replace =TRUE)
        result_clus=training_cases[result,"cluster_type"]
        this_iter_result[[i]]=data.frame(query_cluster=training_cases[divisions_out[[i]],"cluster_type"],result=result_clus)
      }
      all_results[[sim_i]]=do.call(rbind,this_iter_result)
      }
      
      all_results_frame=do.call(rbind,all_results)
      wrong_pred_all_results_frame=all_results_frame[all_results_frame[,1]!=all_results_frame[,2],]
      
      wrong_pred_all_results_frame_table=as.data.frame(table(wrong_pred_all_results_frame[,1],wrong_pred_all_results_frame[,2])/num_it)
      
      wrong_pred_all_results_frame_table=wrong_pred_all_results_frame_table[order(wrong_pred_all_results_frame_table$Freq),]
      wrong_pred_all_results_frame_table$ratio=wrong_pred_all_results_frame_table$Freq/sum(wrong_pred_all_results_frame_table$Freq)
      
      simulated_prediction[[loop_type]]=wrong_pred_all_results_frame_table
      gc()
  }   
  
  # comparing the significance of wrong classification with accountance of the case number
  
  comparison_summary=list()
  for(loop_type in names(most_freq)){
  most_freq[[loop_type]]$type=paste(most_freq[[loop_type]][,1],most_freq[[loop_type]][,2],sep="")
  simulated_prediction[[loop_type]][,1]=as.character(simulated_prediction[[loop_type]][,1]); 
  simulated_prediction[[loop_type]][,2]=as.character(simulated_prediction[[loop_type]][,2])

  simulated_prediction[[loop_type]]$type=paste(simulated_prediction[[loop_type]][,1],simulated_prediction[[loop_type]][,2],sep="")
  this_fr=most_freq[[loop_type]]    
  Fvalue=apply(this_fr,1,function(x){
    print(x[["ratio"]])
    print(simulated_prediction[[loop_type]][,"type"]==x[["type"]])
    tt=simulated_prediction[[loop_type]][,"type"]==x[["type"]]
    as.numeric(as.character(x[["ratio"]]))/as.numeric(as.character(simulated_prediction[[loop_type]][tt,"ratio"]))
    })   
  
  simu_ratio=apply(this_fr,1,function(x){
    print(x[["ratio"]])
    print(simulated_prediction[[loop_type]][,"type"]==x[["type"]])
    tt=simulated_prediction[[loop_type]][,"type"]==x[["type"]]
    as.numeric(as.character(simulated_prediction[[loop_type]][tt,"ratio"]))
  })
  
  simu_freq=apply(this_fr,1,function(x){
    print(x[["ratio"]])
    print(simulated_prediction[[loop_type]][,"type"]==x[["type"]])
    tt=simulated_prediction[[loop_type]][,"type"]==x[["type"]]
    as.numeric(as.character(simulated_prediction[[loop_type]][tt,"Freq"]))
  }) 
      
  this_fr$F_stat=Fvalue
  this_fr$simu_freq=simu_freq
  this_fr$simu_ratio=simu_ratio
  this_fr$freq_ratio=this_fr$Freq/this_fr$simu_freq
  this_fr=this_fr[order(this_fr$freq_ratio,decreasing=TRUE),]
  comparison_summary[[loop_type]]=this_fr
  }
      
  comparison_summary_frame=do.call(rbind,comparison_summary)    
  comparison_summary_frame=comparison_summary_frame[order(comparison_summary_frame$freq_ratio,decreasing=TRUE),]
  #comparison_summary_frame=comparison_summary_frame[comparison_summary_frame$freq_ratio>0.8,]
  #comparison_summary_frame_major=comparison_summary_frame[comparison_summary_frame$Freq>10,]
  comparison_summary_frame_major=comparison_summary_frame[order(comparison_summary_frame$Var1),]
  comparison_summary_frame_major=comparison_summary_frame_major[comparison_summary_frame_major$Freq>5,]
  
  loop_types=(lapply(strsplit(as.character(comparison_summary_frame_major$Var1),"-"),function(x){paste(x[1:2],collapse="-")}))
  # find the most popular cluster 
  splitted_by_loop_types=split(comparison_summary_frame_major,unlist(loop_types))
  
  wrong_case_summary_for_all_loops=list()
  for(each_l in names(splitted_by_loop_types)){
  H1_related=comparison_summary_frame_major[grepl(each_l,comparison_summary_frame_major$Var1),]
  # most prevalent cluster 
  if(each_l=="L3-9"){
    most_p="L3-9-cis7-1"
  }else{
  most_p=paste(c(each_l,"-1"),collapse="")
  }
  H1_cluster_one_recovery_fail=H1_related[grepl(most_p,H1_related$Var1),]
  H1_cluster_one_precision_fail=H1_related[grepl(most_p,H1_related$Var2),]
  H1_cluster_one_recovery_fail$error_type=rep("cluster_1_recovery",dim(H1_cluster_one_recovery_fail)[1])
  H1_cluster_one_precision_fail$error_type=rep("cluster_1_precision",dim(H1_cluster_one_precision_fail)[1])
  other_types=H1_related[!(grepl(most_p,H1_related$Var1) |grepl(most_p,H1_related$Var2)),]
  other_types$error_type=rep("less_popular_cluster_error",dim(other_types)[1])
  all_annotated=rbind(rbind(other_types,H1_cluster_one_precision_fail),H1_cluster_one_recovery_fail)

  wrong_case_summary=as.data.frame(t(as.data.frame(lapply(split(all_annotated,all_annotated$error_type),function(x){
    c(sum(x$Freq),sum(x$simu_freq))
  }))))
  wrong_case_summary$original_wrong_case=wrong_case_summary$V1
  wrong_case_summary$V1=wrong_case_summary$V1/max(wrong_case_summary$V1)*10
  
  wrong_case_summary$error_type=row.names(wrong_case_summary)
  wrong_case_summary_for_all_loops[[each_l]][[1]]=data.frame(wrong_case_summary,loop=rep(each_l,dim(wrong_case_summary)[1]))
  wrong_case_summary_for_all_loops[[each_l]][[2]]=data.frame(all_annotated,loop=rep(each_l,dim(all_annotated)[1]))
  }
  all_annotated=do.call(rbind,lapply(wrong_case_summary_for_all_loops,function(x){x[[2]]}))
  wrong_case_summary=do.call(rbind,lapply(wrong_case_summary_for_all_loops,function(x){x[[1]]}))
     
  ggplot(all_annotated, aes(x = freq_ratio,fill=error_type,color=error_type)) +
    geom_density()+facet_wrap(~loop,scales="free")+theme(axis.text.x = element_text(size=9,angle=90, hjust=1),plot.title = element_text(hjust=0.5))+xlab("frequency ratio")+ggtitle("Ratio distribution colored by error type and separate by loop type")
  ggsave("ratio_distribution.pdf", plot = last_plot(), device = NULL, path = NULL,
         width = 7.5, height = 5, units = "in", limitsize = TRUE)
  
  ggplot(all_annotated)+
    geom_bar(data=wrong_case_summary,aes(x=error_type,y=V1,fill=error_type),inherit.aes = FALSE,stat = "identity")+
    geom_text(data=wrong_case_summary,aes(x=error_type,y=V1,label=original_wrong_case), position=position_dodge(width=0.9), vjust=-0.25)+
    geom_point(aes(x=error_type,y=freq_ratio,color=freq_ratio))+theme(axis.text.x=element_blank())+
    ggtitle("The BLAST error type bias compared to random assignment")+
    facet_wrap(~loop)+ylim(0,12)+ylab("ratio of BLAST error vs random assignment simulation")+theme(plot.title=(element_text(size = 11)))
  ggsave("random_simulation.pdf", plot = last_plot(), device = NULL, path = NULL,
         width = 7.5, height = 5, units = "in", limitsize = TRUE)
  
  
  # code the similarity comparison with x be the mean similarity between the cases in one cluster and its corresponding sequence pair of the most
  
